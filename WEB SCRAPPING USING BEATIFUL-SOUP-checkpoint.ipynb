{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup - It will be used to parse the source code and to extract the required data from the parent structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests - It will be used to get request to the web page server to get the source code pf the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from requests) (1.25.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:-\n",
    "    Write a python program to display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPLAY ALL THE HEADER TAGS \n",
      "\n",
      "\n",
      "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">Main Page</h1> \n",
      "\n",
      "Text from the header tags :-\n",
      "Main Page \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "From today's featured article \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Did you know ... \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "In the news \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "On this day \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfl-h2\"><span id=\"From_today.27s_featured_list\"></span><span class=\"mw-headline\" id=\"From_today's_featured_list\">From today's featured list</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "From today's featured list \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Today's featured picture \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Other areas of Wikipedia \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Wikipedia's sister projects \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Wikipedia languages \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2>Navigation menu</h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Navigation menu \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-personal-label\">\n",
      "<span>Personal tools</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Personal tools\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-namespaces-label\">\n",
      "<span>Namespaces</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Namespaces\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-variants-label\">\n",
      "<span>Variants</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Variants\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-views-label\">\n",
      "<span>Views</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Views\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-cactions-label\">\n",
      "<span>More</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "More\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Search\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-navigation-label\">\n",
      "<span>Navigation</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Navigation\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-interaction-label\">\n",
      "<span>Contribute</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Contribute\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-tb-label\">\n",
      "<span>Tools</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Tools\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-coll-print_export-label\">\n",
      "<span>Print/export</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Print/export\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-wikibase-otherprojects-label\">\n",
      "<span>In other projects</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "In other projects\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-lang-label\">\n",
      "<span>Languages</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Languages\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_1(url):\n",
    "    # Writing a request to fetch the data from the web page \n",
    "    page = requests.get(url)\n",
    "    # Parsing \n",
    "    bs = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Finding all the header tags\n",
    "    header_tags = bs.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "    # Creating a loop to print the headers found in the html structure\n",
    "    print('DISPLAY ALL THE HEADER TAGS','\\n\\n')\n",
    "    for h in header_tags:\n",
    "        # Printing the header tags\n",
    "        print(h,'\\n')\n",
    "        # Printing the text contained in the header tags\n",
    "        print('Text from the header tags :-')\n",
    "        print(h.text,'\\n')\n",
    "        print('---------------------------------------------------------------------------------------------------------------')\n",
    "# Calling the function while specifying the URL\n",
    "function_1(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:-\n",
    "    Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Release_Year</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Genre</th>\n",
       "      <th>IMDB_Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>142 min</td>\n",
       "      <td>Drama</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>175 min</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>152 min</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>202 min</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>96 min</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Eternal Sunshine of the Spotless Mind</td>\n",
       "      <td>(2004)</td>\n",
       "      <td>108 min</td>\n",
       "      <td>Drama, Romance, Sci-Fi</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Amélie</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>122 min</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Snatch</td>\n",
       "      <td>(2000)</td>\n",
       "      <td>104 min</td>\n",
       "      <td>Comedy, Crime</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Requiem for a Dream</td>\n",
       "      <td>(2000)</td>\n",
       "      <td>102 min</td>\n",
       "      <td>Drama</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>American Beauty</td>\n",
       "      <td>(1999)</td>\n",
       "      <td>122 min</td>\n",
       "      <td>Drama</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Movie Release_Year  Runtime  \\\n",
       "Rank                                                                \n",
       "1                  The Shawshank Redemption       (1994)  142 min   \n",
       "2                             The Godfather       (1972)  175 min   \n",
       "3                           The Dark Knight       (2008)  152 min   \n",
       "4                    The Godfather: Part II       (1974)  202 min   \n",
       "5                              12 Angry Men       (1957)   96 min   \n",
       "...                                     ...          ...      ...   \n",
       "96    Eternal Sunshine of the Spotless Mind       (2004)  108 min   \n",
       "97                                   Amélie       (2001)  122 min   \n",
       "98                                   Snatch       (2000)  104 min   \n",
       "99                      Requiem for a Dream       (2000)  102 min   \n",
       "100                         American Beauty       (1999)  122 min   \n",
       "\n",
       "                       Genre IMDB_Rating  \n",
       "Rank                                      \n",
       "1                      Drama         9.3  \n",
       "2               Crime, Drama         9.2  \n",
       "3       Action, Crime, Drama         9.0  \n",
       "4               Crime, Drama         9.0  \n",
       "5               Crime, Drama         9.0  \n",
       "...                      ...         ...  \n",
       "96    Drama, Romance, Sci-Fi         8.3  \n",
       "97           Comedy, Romance         8.3  \n",
       "98             Comedy, Crime         8.3  \n",
       "99                     Drama         8.3  \n",
       "100                    Drama         8.3  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_2(url):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    # Parsing\n",
    "    bs = BeautifulSoup(page.content, 'html.parser') \n",
    "    \n",
    "    # Find all the tags containing the movie names\n",
    "    movies = bs.find_all('h3', class_='lister-item-header')\n",
    "    \n",
    "    movie_title = [t.text for m in movies for t in m.find_all('a')]\n",
    "    \n",
    "    release_year = [y.text for m in movies for y in m.find_all('span', class_='lister-item-year text-muted unbold')]\n",
    "    \n",
    "    #parts = bs.find_all('p', class_='text_muted')\n",
    "    \n",
    "    #certificate = [c.text for p in parts for c in p.find_all('span', class_='certificate')]\n",
    "    \n",
    "    runtime = [r.text for p in bs.find_all('p', class_='text-muted') for r in p.find_all('span', class_='runtime')]\n",
    "    \n",
    "    genre = [g.text.strip() for p in bs.find_all('p',  class_='text-muted') for g in p.find_all('span', class_='genre')]\n",
    "    \n",
    "    imdb_rating = [i.text for b in bs.find_all('div', class_='inline-block ratings-imdb-rating') for i in b.find_all('strong')]\n",
    "    \n",
    "    #metascore = [s.text for b in bs.find_all('div', class_='inline-block ratings-metascore') for s in b.find_all('span', class_='metascore')] \n",
    "    \n",
    "    #votes = [v.text for b in bs.find_all('span', class_='text-muted') for v in b.find_all(name='nv')]\n",
    "    \n",
    "    Rank = range(1,101)\n",
    "    #Building the data frame\n",
    "    Top_100_movies = pd.DataFrame({\n",
    "        'Rank' : Rank,\n",
    "        'Movie' : movie_title,\n",
    "        'Release_Year' : release_year,\n",
    "        #'Certificate' : certificate,\n",
    "        'Runtime' : runtime,\n",
    "        'Genre' : genre,\n",
    "        'IMDB_Rating' : imdb_rating,\n",
    "        #'MetaScore' : metascore,\n",
    "        #'Votes' : votes,\n",
    "        #'Director' : director,\n",
    "        #'Stars' : stars\n",
    "    })\n",
    "    \n",
    "    Top_100_movies.set_index('Rank', inplace=True)\n",
    "    # Saving the DataFrame to CSV\n",
    "    Top_100_movies.to_csv('IMDB top 100 movies.csv')\n",
    "    \n",
    "    return(Top_100_movies)\n",
    "\n",
    "function_2(\"https://www.imdb.com/search/title/?count=100&groups=top_1000&sort=user_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:-\n",
    "    Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movies</th>\n",
       "      <th>Release_Year</th>\n",
       "      <th>IMDB_Rating</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>(2006)</td>\n",
       "      <td>8.2</td>\n",
       "      <td>167 min</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>(2009)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>170 min</td>\n",
       "      <td>Comedy, Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>(2007)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>165 min</td>\n",
       "      <td>Drama, Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dil Chahta Hai</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>8.1</td>\n",
       "      <td>183 min</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Swades: We, the People</td>\n",
       "      <td>(2004)</td>\n",
       "      <td>8.2</td>\n",
       "      <td>210 min</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Wake Up Sid</td>\n",
       "      <td>(2009)</td>\n",
       "      <td>7.6</td>\n",
       "      <td>138 min</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Rangeela</td>\n",
       "      <td>(1995)</td>\n",
       "      <td>7.5</td>\n",
       "      <td>142 min</td>\n",
       "      <td>Comedy, Drama, Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Shatranj Ke Khilari</td>\n",
       "      <td>(1977)</td>\n",
       "      <td>7.7</td>\n",
       "      <td>129 min</td>\n",
       "      <td>Comedy, Drama, History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Pyaar Ka Punchnama</td>\n",
       "      <td>(2011)</td>\n",
       "      <td>7.7</td>\n",
       "      <td>149 min</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Ek Hasina Thi</td>\n",
       "      <td>(2004)</td>\n",
       "      <td>7.5</td>\n",
       "      <td>120 min</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Movies Release_Year IMDB_Rating  Runtime  \\\n",
       "Rank                                                             \n",
       "1            Rang De Basanti       (2006)         8.2  167 min   \n",
       "2                   3 Idiots       (2009)         8.4  170 min   \n",
       "3           Taare Zameen Par       (2007)         8.4  165 min   \n",
       "4             Dil Chahta Hai       (2001)         8.1  183 min   \n",
       "5     Swades: We, the People       (2004)         8.2  210 min   \n",
       "...                      ...          ...         ...      ...   \n",
       "96               Wake Up Sid       (2009)         7.6  138 min   \n",
       "97                  Rangeela       (1995)         7.5  142 min   \n",
       "98       Shatranj Ke Khilari       (1977)         7.7  129 min   \n",
       "99        Pyaar Ka Punchnama       (2011)         7.7  149 min   \n",
       "100            Ek Hasina Thi       (2004)         7.5  120 min   \n",
       "\n",
       "                       Genre  \n",
       "Rank                          \n",
       "1       Comedy, Crime, Drama  \n",
       "2              Comedy, Drama  \n",
       "3              Drama, Family  \n",
       "4     Comedy, Drama, Romance  \n",
       "5                      Drama  \n",
       "...                      ...  \n",
       "96    Comedy, Drama, Romance  \n",
       "97    Comedy, Drama, Musical  \n",
       "98    Comedy, Drama, History  \n",
       "99    Comedy, Drama, Romance  \n",
       "100    Crime, Drama, Mystery  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_3(url):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page.content, 'html.parser') \n",
    "    \n",
    "    # Scrape all the titles\n",
    "    Movies = [m.text for h3 in bs.find_all('h3', class_='lister-item-header') for m in h3.find_all('a')]\n",
    "    \n",
    "    # Scrape all the release year\n",
    "    Release_Year = [y.text for h3 in bs.find_all('h3', class_='lister-item-header') for y in h3.find_all('span', class_='lister-item-year')]\n",
    "    \n",
    "    # Scrape the rating\n",
    "    IMDB_Rating = [i.text for d in bs.find_all('div', class_='ipl-rating-star small') for i in d.find_all('span', class_='ipl-rating-star__rating')]\n",
    "    \n",
    "    # Scrape all the runtime\n",
    "    Runtime = [r.text for p in bs.find_all('p', class_='text-muted text-small') for r in p.find_all('span', class_='runtime')]\n",
    "    \n",
    "    # Scrape all the genres\n",
    "    Genre = [g.text.strip() for p in bs.find_all('p', class_='text-muted text-small') for g in p.find_all('span', class_='genre')]\n",
    "    \n",
    "    Rank = range(1,101)\n",
    "    #Building the data frame\n",
    "    Top_100_Indian_movies = pd.DataFrame({\n",
    "        'Rank' : Rank,\n",
    "        'Movies' : Movies,\n",
    "        'Release_Year' : Release_Year,\n",
    "        'IMDB_Rating' : IMDB_Rating,\n",
    "        'Runtime' : Runtime,\n",
    "        'Genre' : Genre\n",
    "    })\n",
    "\n",
    "    Top_100_Indian_movies.set_index('Rank', inplace=True)\n",
    "    # Saving the DataFrame to CSV\n",
    "    Top_100_Indian_movies.to_csv('IMDB top 100 Indian movies.csv')\n",
    "    \n",
    "    return(Top_100_Indian_movies)\n",
    "\n",
    "function_3('https://www.imdb.com/list/ls009997493/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:-\n",
    "    Write a python program to scrap book name, author name, genre and book review of any 5 books from ‘www.bookpage.com’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page number ----> 523 \n",
      "\n",
      "\n",
      "Any 5 books from the page:- \n",
      "\n",
      "\n",
      "1 ---->\n",
      "book_name : If You Could Be Mine\n",
      "author_name: Sara Farizan\n",
      "genre: Children's\n",
      "review: \n",
      "\n",
      "A confession: I picked up If You Could Be Mine knowing only that it was about two teenage girls in love in Tehran. While homosexuality is a crime in Iran, transsexuals are tolerated if not enthusiastically embraced, so one of the girls contemplates sex change surgery for the chance to love without risk of death. I assumed the book would be grim and possibly preachy—how else could you tell a story with so much at stake? Thankfully, I could not have been more wrong. If You Could Be Mine is at once dazzling and funny and heartbreaking and wise.Sahar and Nasrin have been best friends—and girlfriends—since early childhood. When Nasrin’s parents arrange a marriage for her, Sahar considers changing her gender in order to try to stop the wedding. The people she meets at a transgender support group question her motivation, but reluctantly offer their help. When one who comes to meet her at an underground gay bar is openly hostile to the crowd—it’s not just elitism but Muslim law that separates gay and transgender people—Sahar’s gay cousin Ali intervenes. When the woman explains she came to deliver hormones to Sahar, “Ali looks at me like I have just told him I have killed Britney Spears, Madonna, and Lady Gaga.”A girl considers extreme lengths for love in Farizan's debut.Things only get more difficult from there. Sahar’s relationship with Nasrin suffers as the wedding approaches, and at home she tries to wake her widowed father from a five-year period of mourning and detachment. Eventually she begins to carve out a new life for herself, and a new relationship with Nasrin.This is Sara Farizan’s first novel, and what a debut it is. The Iran revealed through the eyes of her teenaged characters is a place of oppression and great risk, but the Ayatollahs are viewed as little more than cranky grandfathers. The West is regarded with a mix of awe at the freedom allowed there and disgust that it is so unappreciated.Sahar and Nasrin’s circumstances differ from those of most Americans in drastic ways, but their love, heartbreak and redemption will resonate with anyone. If You Could Be Mine is a beautiful, compassionate, must-read novel.\n",
      "\n",
      "\n",
      "2 ---->\n",
      "book_name : More Than This\n",
      "author_name: Patrick Ness\n",
      "genre: Children's\n",
      "review: \n",
      "\n",
      "Seth drowns in a furious ocean, his body battered by freezing waves and sharp rocks. But as his consciousness gradually returns, he finds himself in a world that’s both foreign and eerily familiar. It appears to be a long-abandoned version of his childhood hometown, the British village full of painful memories that his family left eight years ago to start a new life far away. Strangest of all, this alternate, desolate world seems to respond directly to Seth’s thoughts, putting everything from supplies to companions in front of him just as he needs them.As Seth and two other mismatched teens band together to avoid a terrifying menace, all three are haunted by frighteningly realistic dreams of their previous lives. Issues of forbidden love, unwavering friendship, complex family dynamics, the difference between childhood and adulthood, violent abuse and teen suicide dovetail as the three survivors gradually figure out where they really are . . . and what they might be able to do about it.Artsy, creepy and full of psychological suspense, More Than This from Carnegie Medal-winning author Patrick Ness combines the science-fiction/thriller aspects of Robison Wells’ Variant with the surreal, trauma-induced alternate realities of Andrew Smith’s The Marbury Lens. As readers familiar with the Chaos Walking trilogy know, Ness specializes in writing post-apocalyptic worlds where things are rarely as they seem. When the truth—or what might be the truth—is finally revealed, the answers are both fitting and surprising. The dizzying ending brings the characters to the narrow edge between inevitable outcomes and hope for second chances—and challenges readers to form their own conclusions.\n",
      "\n",
      "\n",
      "3 ---->\n",
      "book_name : Bluffton\n",
      "author_name: Matt Phelan\n",
      "genre: YA\n",
      "review: \n",
      "\n",
      "As Bluffton unfolds, Henry Harrison is facing the prospect of a boring summer in sleepy, ordinary Muskegon, Michigan. When a troupe of vaudeville performers arrives, he’s enthralled, especially when one of the actors, a kid his own age named Buster Keaton, turns out to be as much of a baseball nut as he is—maybe even more so.\n",
      "The year is 1908 and 13-year-old Buster, known as “the human mop” for his ability to take a fall, is already an old hand at performing. Buster is more interested in having a lazy, normal summer filled with baseball, away from the pressures of the stage. And, since baseball is what the boys have in common, everyone gets along. “Baseball does that,” comments Henry. But underneath summer pastimes, the two boys face larger issues of what their futures will hold. Summers, and the innocence of childhood, don’t last forever.\n",
      "Matt Phelan, who won the Scott O’Dell Award for Historical Fiction for his first graphic novel, The Storm in the Barn, effectively evokes the transient magic of summer in this poignant, beautifully illustrated story of two boys from very different worlds.\n",
      "Inspired by the Actors’ Colony at Bluffton, which was founded by Buster’s father, Joe Keaton, and existed from 1908-1938, Bluffton introduces young readers to one of the icons in film history. Phelan is clearly a fan, and his enthusiasm and respect for Buster Keaton are evident throughout. Recommending Keaton films to young readers, Phelan says in his author’s note that Buster Keaton was “a true genius,” whose “hilarious, breathtaking, innovative” films will change the way viewers think about the age of silent movies. (Phelan is not alone: In 2007, The General was named by the American Film Institute as number 18 on a list of the top 100 American films ever made.)\n",
      "By showing us Buster Keaton as a boy trying to balance a public self with private dreams, Phelan’s book is a wonderful gift for readers of all ages.\n",
      "\n",
      "\n",
      "4 ---->\n",
      "book_name : The Man with the Violin\n",
      "author_name: Kathy Stinson\n",
      "genre: YA Fiction\n",
      "review: \n",
      "\n",
      "Observant child versus oblivious adult: It’s a classic contrast. In Kathy Stinson’s delightful new picture book, The Man with the Violin, the opposition serves as the basis for the story of a mom-in-a-hurry who hears but doesn’t listen and her curious, receptive son—a little boy named Dylan, who’s wise beyond his years and in tune with the world, as sensitive to his surroundings as, well, a violin.\n",
      "On a winter day, as mother and son make their way through a crowded metro station, Dylan’s attention is arrested by the sound of music. Its source: a nondescript man with a violin who plays with his eyes closed, clearly transported by the tune that issues from his instrument. The sound “makes Dylan’s skin hu-u-mmm,” and he, too, is transported. In one picture, he floats in mid-air, lifted by the song’s power—and surrounded by puzzled onlookers. Dylan wants to linger and listen. He begs his mother to stop, but she refuses. She sweeps Dylan onto an escalator and away.\n",
      "Later, at home, the unimaginable occurs: Dylan hears the same tune on the radio. When he learns the truth about the man responsible for it, he’s ecstatic. His mother soon realizes that she should’ve taken Dylan’s advice and opened her ears. Together, they share a musical moment in the kitchen—a sweet note for the story’s end.\n",
      "Stinson’s lovely book was inspired by an anonymous performance given by renowned violinist Joshua Bell in a Washington, D.C., subway station in 2007. Almost all of the busy rush-hour passengers ignored the violinist’s beautiful music—except for the children. As the Washington Post reported, “Every single time a child walked past, he or she tried to stop and watch. And every single time, a parent scooted the kid away.\n",
      "Bell himself contributes a postscript to The Man with the Violin, bringing the story full circle.\n",
      "Dušan Petri?i?’s fanciful illustrations play up the contrast between kid and adult. He portrays the metro as a gray point of transit teeming with intent, focused grown-ups and filled with white noise—a symphony of meaningless sound that takes the shape of jagged, zig-zag lines and spiky lightning bolts. This cacophony literally hangs in the air and competes with the violinist, whose music Petri?i? depicts as a cascading ribbon of color. \n",
      "There’s plenty to ponder in this melodious tale. It’s a story that’s bound to get kids thinking—about the importance of listening. And, of course, the power of music.\n",
      "\n",
      "\n",
      "5 ---->\n",
      "book_name : Imperfect Spiral\n",
      "author_name: Debbie Levy\n",
      "genre: Children's Chapter\n",
      "review: \n",
      "\n",
      "All Danielle wanted was a quiet, peaceful summer, and her babysitting job provided her with just that. Five-year-old Humphrey was adorable, funny and better company than the friends from whom she seemed to be growing apart. And then came the accident, a split second during their walk home from the park: Humphrey ran into the road and was hit and killed by a passing car, and Danielle can’t help but feel responsible.A devastating accident leaves a girl scrambling for answers.Guilt and grief consume Danielle as she tries to remember the good times with Humphrey, the afternoons of make-believe and nights of Popsicles, but in the aftermath of the accident, her sleepy neighborhood buzzes with controversy. Some members of the community think a lack of streetlights caused the crash and lobby for improvement, while others are hung up on the fact that the driver of the car was an illegal immigrant. What no one wants to talk about, Danielle realizes, is the smart little boy who was lost that night.When her parents put her in therapy to deal with the loss, Danielle begins to realize that she was having problems long before the accident, and if she wants to get past them and honor Humphrey's memory, she's going to have to speak her mind in the neighborhood’s debates.Author Debbie Levy’s depiction of loss in Imperfect Spiral is powerful, and equally as compelling are her frequent flashbacks of Danielle and Humphrey’s time together. Levy has created an incredibly nuanced relationship between the two, showing that the most important relationships can form outside traditional boundaries like age groups and family ties. Within these memories she also explores Humphrey’s family and the complicated mix of a parent’s love and expectations.The novel sometimes lags under the weight of all the issues it attempts to address. Danielle deals not only with her panic attacks and feelings of loss, but also her feelings on illegal immigration and the strain between her parents and brother, and the story becomes exhausting for both her and the reader. Likewise, the addition of a love interest feels squeezed in, and while the relationship’s development is genuine, it also feels rushed.Despite its flaws, Imperfect Spiral is a powerful book that stays with you long after you’ve read the final page. It’s a story of love and loss that distinguishes itself from the flood of YA books tackling those topics by challenging how we define family and who you can count among your friends.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_4(url):\n",
    "    \n",
    "    #pick any random page number\n",
    "    ran = random.randint(1,1000)\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page \n",
    "    if ran == 1:\n",
    "        page = requests.get(url + '/reviews')\n",
    "    else:\n",
    "        page = requests.get(url + '/reviews?page=' + str(ran))\n",
    "    print('page number ---->',ran,'\\n\\n')\n",
    "    \n",
    "    # Parsing \n",
    "    bs = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Finding all book names and storing them into a list\n",
    "    book_name = [n.text for b in bs.find_all('h4', class_='italic') for n in b.find_all('a')]\n",
    "    #print(book_name)\n",
    "    \n",
    "    # Finding all author names and storing them into a list\n",
    "    author_name = [a.text.strip('\\n') for a in bs.find_all('p', class_='sans bold')]\n",
    "    #print(author_name)\n",
    "    \n",
    "    # Finding all genres and storing them into a list\n",
    "    genre = [g.text.strip() for i in bs.find_all('p', class_='genre-links hidden-phone') for g in i.find_all('a')]\n",
    "    #print(genre)\n",
    "    \n",
    "    # Finding all reviews along with their urls and storing them into a list\n",
    "    reviews = [r.get('href') for b in bs.find_all('div', class_='read-full') for r in b.find_all('a')]\n",
    "    #print(reviews)\n",
    "    \n",
    "    # We need only the body of the article, so creating a loop for getting full reviews\n",
    "    review = []\n",
    "    for r in reviews:\n",
    "        review_page = requests.get('https://www.bookpage.com' + r)\n",
    "        soup = BeautifulSoup(review_page.content,'html.parser')\n",
    "        read_reviews = soup.find_all('div', class_='article-body')\n",
    "        for rr in read_reviews:\n",
    "            review.append(rr.text)\n",
    "            \n",
    "    #print(review)\n",
    "    \n",
    "    #Printing any 5 random books\n",
    "    print('Any 5 books from the page:-','\\n\\n')\n",
    "\n",
    "    # Choosing any 5 random books from the selected random page\n",
    "    ran_index = set()\n",
    "    while len(ran_index) < 5:\n",
    "        ran_index.add(random.randint(0,9))\n",
    "        \n",
    "    for i,index in enumerate(ran_index,start=1):\n",
    "        print(i,'---->')\n",
    "        print('book_name :', book_name[index])\n",
    "        print('author_name:', author_name[index])\n",
    "        print('genre:', genre[index])\n",
    "        try:\n",
    "            print('review:', review[index])\n",
    "        except:\n",
    "            print('review:', 'NOT AVAILABLE')\n",
    "        \n",
    "# Calling the function while specifying the URL\n",
    "function_4('http://www.bookpage.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5:-\n",
    "    Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:                               \n",
    "    i)   Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.                                   ii)  Top 10 ODI Batsmen in men along with the records of their team and rating.    \n",
    "    iii) Top 10 ODI bowlers along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in men’s cricket ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>matches</th>\n",
       "      <th>points</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG</td>\n",
       "      <td>44</td>\n",
       "      <td>5,405</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IND</td>\n",
       "      <td>52</td>\n",
       "      <td>6,102</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NZ</td>\n",
       "      <td>32</td>\n",
       "      <td>3,716</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUS</td>\n",
       "      <td>39</td>\n",
       "      <td>4,344</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SA</td>\n",
       "      <td>31</td>\n",
       "      <td>3,345</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PAK</td>\n",
       "      <td>35</td>\n",
       "      <td>3,490</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BAN</td>\n",
       "      <td>37</td>\n",
       "      <td>3,366</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SL</td>\n",
       "      <td>39</td>\n",
       "      <td>3,297</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WI</td>\n",
       "      <td>46</td>\n",
       "      <td>3,402</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AFG</td>\n",
       "      <td>31</td>\n",
       "      <td>1,844</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     team matches points rating\n",
       "rank                           \n",
       "1     ENG      44  5,405    123\n",
       "2     IND      52  6,102    117\n",
       "3      NZ      32  3,716    116\n",
       "4     AUS      39  4,344    111\n",
       "5      SA      31  3,345    108\n",
       "6     PAK      35  3,490    100\n",
       "7     BAN      37  3,366     91\n",
       "8      SL      39  3,297     85\n",
       "9      WI      46  3,402     74\n",
       "10    AFG      31  1,844     59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Top 10 ODI Batsmen in men ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>870</td>\n",
       "      <td>911 v England, 12/07/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>842</td>\n",
       "      <td>885 v Sri Lanka, 06/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>837</td>\n",
       "      <td>846 v Sri Lanka, 20/10/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>818</td>\n",
       "      <td>841 v Bangladesh, 05/06/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aaron Finch</td>\n",
       "      <td>AUS</td>\n",
       "      <td>791</td>\n",
       "      <td>798 v England, 25/06/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Francois du Plessis</td>\n",
       "      <td>SA</td>\n",
       "      <td>790</td>\n",
       "      <td>820 v Australia, 06/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>773</td>\n",
       "      <td>880 v Pakistan, 26/01/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kane Williamson</td>\n",
       "      <td>NZ</td>\n",
       "      <td>765</td>\n",
       "      <td>799 v India, 09/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>755</td>\n",
       "      <td>813 v Sri Lanka, 10/03/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>754</td>\n",
       "      <td>777 v Australia, 21/06/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   player team rating            career_best_rating\n",
       "rank                                                               \n",
       "1             Virat Kohli  IND    870     911 v England, 12/07/2018\n",
       "2            Rohit Sharma  IND    842   885 v Sri Lanka, 06/07/2019\n",
       "3              Babar Azam  PAK    837   846 v Sri Lanka, 20/10/2017\n",
       "4             Ross Taylor   NZ    818  841 v Bangladesh, 05/06/2019\n",
       "5             Aaron Finch  AUS    791     798 v England, 25/06/2019\n",
       "6     Francois du Plessis   SA    790   820 v Australia, 06/07/2019\n",
       "7            David Warner  AUS    773    880 v Pakistan, 26/01/2017\n",
       "8         Kane Williamson   NZ    765       799 v India, 09/07/2019\n",
       "9         Quinton de Kock   SA    755   813 v Sri Lanka, 10/03/2019\n",
       "10         Jonny Bairstow  ENG    754   777 v Australia, 21/06/2018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Top 10 ODI bolwingmen in men ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>722</td>\n",
       "      <td>770 v West Indies, 22/06/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>708</td>\n",
       "      <td>712 v Ireland, 24/01/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>700</td>\n",
       "      <td>841 v West Indies, 01/11/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>694</td>\n",
       "      <td>694 v West Indies, 25/01/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chris Woakes</td>\n",
       "      <td>ENG</td>\n",
       "      <td>675</td>\n",
       "      <td>676 v New Zealand, 14/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kagiso Rabada</td>\n",
       "      <td>SA</td>\n",
       "      <td>665</td>\n",
       "      <td>724 v England, 29/05/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>660</td>\n",
       "      <td>733 v England, 26/01/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mustafizur Rahman</td>\n",
       "      <td>BAN</td>\n",
       "      <td>658</td>\n",
       "      <td>695 v West Indies, 14/12/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mohammad Amir</td>\n",
       "      <td>PAK</td>\n",
       "      <td>647</td>\n",
       "      <td>663 v Sri Lanka, 02/10/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pat Cummins</td>\n",
       "      <td>AUS</td>\n",
       "      <td>646</td>\n",
       "      <td>729 v Pakistan, 12/06/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 player team rating             career_best_rating\n",
       "rank                                                              \n",
       "1           Trent Boult   NZ    722  770 v West Indies, 22/06/2019\n",
       "2      Mujeeb Ur Rahman  AFG    708      712 v Ireland, 24/01/2021\n",
       "3        Jasprit Bumrah  IND    700  841 v West Indies, 01/11/2018\n",
       "4          Mehedi Hasan  BAN    694  694 v West Indies, 25/01/2021\n",
       "5          Chris Woakes  ENG    675  676 v New Zealand, 14/07/2019\n",
       "6         Kagiso Rabada   SA    665      724 v England, 29/05/2017\n",
       "7        Josh Hazlewood  AUS    660      733 v England, 26/01/2018\n",
       "8     Mustafizur Rahman  BAN    658  695 v West Indies, 14/12/2018\n",
       "9         Mohammad Amir  PAK    647    663 v Sri Lanka, 02/10/2019\n",
       "10          Pat Cummins  AUS    646     729 v Pakistan, 12/06/2019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_5(url):\n",
    "    \n",
    "    #PART 1:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page1 = requests.get(url + '/rankings/mens/team-rankings/odi')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page1.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "    \n",
    "    # Scraping Country names\n",
    "    team = [t.text for b in bs.find_all('tbody') for t in b.find_all('span', class_='u-show-phablet')]\n",
    "    #print(team)\n",
    "    \n",
    "    # Scraping the Matches\n",
    "    matches_england = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for me in b.find_all('td', class_='rankings-block__banner--matches'):\n",
    "            matches_england.append(me.text)\n",
    "    #print(matches_england)\n",
    "    \n",
    "    matches = [m.text for b in bs.find_all('tr', class_='table-body') for m in b.find_all('td', class_='table-body__cell u-center-text')]\n",
    "    matches = matches[0::2]\n",
    "    matches = matches_england  + matches\n",
    "    #print(matches)\n",
    "    \n",
    "    # Scrapping the points\n",
    "    points_england = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for pe in b.find_all('td', class_='rankings-block__banner--points'):\n",
    "            points_england.append(pe.text)\n",
    "    #print(points_england)\n",
    "    \n",
    "    points = [p.text for b in bs.find_all('tr', class_='table-body') for p in b.find_all('td', class_='table-body__cell u-center-text')]\n",
    "    points = points[1::2]\n",
    "    points = points_england + points\n",
    "    #print(points)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rating_england = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for re in b.find_all('td', class_='rankings-block__banner--rating u-text-right'):\n",
    "            rating_england.append(re.text.strip())\n",
    "    #print(rating_england)\n",
    "    \n",
    "    rating = [r.text for b in bs.find_all('tr', class_='table-body') for r in b.find_all('td', class_='table-body__cell u-text-right rating')]\n",
    "    rating = rating_england + rating\n",
    "    #print(rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_men = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'team' : team[0:10],\n",
    "        'matches' : matches[0:10],\n",
    "        'points' : points[0:10],\n",
    "        'rating' : rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_men.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI teams in men’s cricket ----->')\n",
    "    display(Top_10_ODI_men)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "    \n",
    "    #PART 2:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page2 = requests.get(url + '/rankings/mens/player-rankings/odi/batting')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page2.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    kohli = []\n",
    "    for k in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        kohli.append(k.text)\n",
    "    #print(kohli)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = kohli + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    india = []\n",
    "    for i in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        india.append(i.text.strip())\n",
    "    #print(india)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = india + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_batsmen = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_batsmen.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI Batsmen in men ----->')\n",
    "    display(Top_10_ODI_batsmen)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "    \n",
    "    #PART 3:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page3 = requests.get(url + '/rankings/mens/player-rankings/odi/bowling')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page3.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    trent = []\n",
    "    for tb in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        trent.append(tb.text)\n",
    "    #print(trent)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = trent + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    nz = []\n",
    "    for n in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        nz.append(n.text.strip())\n",
    "    #print(nz)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = nz + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_bowlingmen = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_bowlingmen.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI bolwingmen in men ----->')\n",
    "    display(Top_10_ODI_bowlingmen)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "        \n",
    "# Calling the function while specifying the URL\n",
    "function_5('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6:-\n",
    "    Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:\n",
    "       i)   Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "       ii)  Top 10 women’s ODI players along with the records of their team and rating.\n",
    "       iii) Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in women’s cricket ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>matches</th>\n",
       "      <th>points</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUS</td>\n",
       "      <td>15</td>\n",
       "      <td>2,436</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IND</td>\n",
       "      <td>15</td>\n",
       "      <td>1,812</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG</td>\n",
       "      <td>14</td>\n",
       "      <td>1,670</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SA</td>\n",
       "      <td>19</td>\n",
       "      <td>2,090</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NZ</td>\n",
       "      <td>15</td>\n",
       "      <td>1,384</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WI</td>\n",
       "      <td>12</td>\n",
       "      <td>1,025</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PAK</td>\n",
       "      <td>15</td>\n",
       "      <td>1,101</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BAN</td>\n",
       "      <td>5</td>\n",
       "      <td>306</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SL</td>\n",
       "      <td>11</td>\n",
       "      <td>519</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IRE</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     team matches points rating\n",
       "rank                           \n",
       "1     AUS      15  2,436    162\n",
       "2     IND      15  1,812    121\n",
       "3     ENG      14  1,670    119\n",
       "4      SA      19  2,090    110\n",
       "5      NZ      15  1,384     92\n",
       "6      WI      12  1,025     85\n",
       "7     PAK      15  1,101     73\n",
       "8     BAN       5    306     61\n",
       "9      SL      11    519     47\n",
       "10    IRE       2     25     13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Top 10 ODI Batswomen in women ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "      <td>834 v New Zealand, 24/02/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>746</td>\n",
       "      <td>765 v India, 02/03/2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>741</td>\n",
       "      <td>741 v New Zealand, 07/10/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>732</td>\n",
       "      <td>797 v England, 28/02/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>723</td>\n",
       "      <td>756 v Australia, 02/03/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>716</td>\n",
       "      <td>738 v Australia, 04/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>691</td>\n",
       "      <td>712 v New Zealand, 25/01/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>691</td>\n",
       "      <td>766 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mithali Raj</td>\n",
       "      <td>IND</td>\n",
       "      <td>687</td>\n",
       "      <td>839 v Australia, 24/12/2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lizelle Lee</td>\n",
       "      <td>SA</td>\n",
       "      <td>681</td>\n",
       "      <td>752 v England, 12/06/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 player team rating             career_best_rating\n",
       "rank                                                              \n",
       "1           Meg Lanning  AUS    749  834 v New Zealand, 24/02/2016\n",
       "2       Stafanie Taylor   WI    746        765 v India, 02/03/2012\n",
       "3          Alyssa Healy  AUS    741  741 v New Zealand, 07/10/2020\n",
       "4       Smriti Mandhana  IND    732      797 v England, 28/02/2019\n",
       "5     Amy Satterthwaite   NZ    723    756 v Australia, 02/03/2017\n",
       "6        Tammy Beaumont  ENG    716    738 v Australia, 04/07/2019\n",
       "7       Laura Wolvaardt   SA    691  712 v New Zealand, 25/01/2020\n",
       "8          Ellyse Perry  AUS    691  766 v West Indies, 11/09/2019\n",
       "9           Mithali Raj  IND    687    839 v Australia, 24/12/2004\n",
       "10          Lizelle Lee   SA    681      752 v England, 12/06/2018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "and\n",
      "\n",
      "\n",
      "Top 10 ODI bolwingwomen in women ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>804</td>\n",
       "      <td>804 v New Zealand, 07/10/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megan Schutt</td>\n",
       "      <td>AUS</td>\n",
       "      <td>735</td>\n",
       "      <td>766 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>711</td>\n",
       "      <td>738 v New Zealand, 30/01/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shabnim Ismail</td>\n",
       "      <td>SA</td>\n",
       "      <td>708</td>\n",
       "      <td>717 v New Zealand, 30/01/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>691</td>\n",
       "      <td>796 v England, 28/02/2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Poonam Yadav</td>\n",
       "      <td>IND</td>\n",
       "      <td>679</td>\n",
       "      <td>680 v West Indies, 06/11/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shikha Pandey</td>\n",
       "      <td>IND</td>\n",
       "      <td>675</td>\n",
       "      <td>696 v South Africa, 11/10/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>666</td>\n",
       "      <td>719 v West Indies, 05/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Anya Shrubsole</td>\n",
       "      <td>ENG</td>\n",
       "      <td>645</td>\n",
       "      <td>655 v Pakistan, 12/12/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>639</td>\n",
       "      <td>650 v England, 22/02/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              player team rating              career_best_rating\n",
       "rank                                                            \n",
       "1      Jess Jonassen  AUS    804   804 v New Zealand, 07/10/2020\n",
       "2       Megan Schutt  AUS    735   766 v West Indies, 11/09/2019\n",
       "3     Marizanne Kapp   SA    711   738 v New Zealand, 30/01/2020\n",
       "4     Shabnim Ismail   SA    708   717 v New Zealand, 30/01/2020\n",
       "5     Jhulan Goswami  IND    691       796 v England, 28/02/2007\n",
       "6       Poonam Yadav  IND    679   680 v West Indies, 06/11/2019\n",
       "7      Shikha Pandey  IND    675  696 v South Africa, 11/10/2019\n",
       "8       Ellyse Perry  AUS    666   719 v West Indies, 05/09/2019\n",
       "9     Anya Shrubsole  ENG    645      655 v Pakistan, 12/12/2019\n",
       "10     Deepti Sharma  IND    639       650 v England, 22/02/2019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Top 10 ODI all rounder in women ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>460</td>\n",
       "      <td>548 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>410</td>\n",
       "      <td>559 v New Zealand, 10/10/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>396</td>\n",
       "      <td>412 v Pakistan, 23/01/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>359</td>\n",
       "      <td>397 v South Africa, 09/10/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>301</td>\n",
       "      <td>308 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dane van Niekerk</td>\n",
       "      <td>SA</td>\n",
       "      <td>297</td>\n",
       "      <td>421 v Sri Lanka, 11/02/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sophie Devine</td>\n",
       "      <td>NZ</td>\n",
       "      <td>289</td>\n",
       "      <td>305 v Australia, 05/10/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>273</td>\n",
       "      <td>307 v Sri Lanka, 21/03/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shikha Pandey</td>\n",
       "      <td>IND</td>\n",
       "      <td>250</td>\n",
       "      <td>262 v South Africa, 14/10/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>232</td>\n",
       "      <td>270 v Sri Lanka, 16/03/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                player team rating              career_best_rating\n",
       "rank                                                              \n",
       "1         Ellyse Perry  AUS    460   548 v West Indies, 11/09/2019\n",
       "2      Stafanie Taylor   WI    410   559 v New Zealand, 10/10/2013\n",
       "3       Marizanne Kapp   SA    396      412 v Pakistan, 23/01/2021\n",
       "4        Deepti Sharma  IND    359  397 v South Africa, 09/10/2019\n",
       "5        Jess Jonassen  AUS    301   308 v West Indies, 11/09/2019\n",
       "6     Dane van Niekerk   SA    297     421 v Sri Lanka, 11/02/2019\n",
       "7        Sophie Devine   NZ    289     305 v Australia, 05/10/2020\n",
       "8       Natalie Sciver  ENG    273     307 v Sri Lanka, 21/03/2019\n",
       "9        Shikha Pandey  IND    250  262 v South Africa, 14/10/2019\n",
       "10     Katherine Brunt  ENG    232     270 v Sri Lanka, 16/03/2019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_6(url):\n",
    "    \n",
    "    #PART 1:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page1 = requests.get(url + '/rankings/womens/team-rankings/odi')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page1.content, 'html.parser')\n",
    "    \n",
    "    # Scrapping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "    \n",
    "     # Scraping Country names\n",
    "    team = [t.text for b in bs.find_all('tbody') for t in b.find_all('span', class_='u-show-phablet')]\n",
    "    #print(team)\n",
    "    \n",
    "    # Scraping the Matches\n",
    "    matches_australia = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for ma in b.find_all('td', class_='rankings-block__banner--matches'):\n",
    "            matches_australia.append(ma.text)\n",
    "    #print(matches_australia)\n",
    "    \n",
    "    matches = [m.text for b in bs.find_all('tr', class_='table-body') for m in b.find_all('td', class_='table-body__cell u-center-text')]\n",
    "    matches = matches[0::2]\n",
    "    matches = matches_australia  + matches\n",
    "    #print(matches)\n",
    "    \n",
    "    # Scrapping the points\n",
    "    points_australia = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for pa in b.find_all('td', class_='rankings-block__banner--points'):\n",
    "            points_australia.append(pa.text)\n",
    "    #print(points_england)\n",
    "    \n",
    "    points = [p.text for b in bs.find_all('tr', class_='table-body') for p in b.find_all('td', class_='table-body__cell u-center-text')]\n",
    "    points = points[1::2]\n",
    "    points = points_australia + points\n",
    "    #print(points)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rating_australia = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for ra in b.find_all('td', class_='rankings-block__banner--rating u-text-right'):\n",
    "            rating_australia.append(ra.text.strip())\n",
    "    #print(rating_australia)\n",
    "    \n",
    "    rating = [r.text for b in bs.find_all('tr', class_='table-body') for r in b.find_all('td', class_='table-body__cell u-text-right rating')]\n",
    "    rating = rating_australia + rating\n",
    "    #print(rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_women = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'team' : team[0:10],\n",
    "        'matches' : matches[0:10],\n",
    "        'points' : points[0:10],\n",
    "        'rating' : rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_women.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI teams in women’s cricket ----->')\n",
    "    display(Top_10_ODI_women)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "    \n",
    "    #PART 2:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page2 = requests.get(url + '/rankings/womens/player-rankings/odi/batting')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page2.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    meg = []\n",
    "    for me in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        meg.append(me.text)\n",
    "    #print(meg)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = meg + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    aus = []\n",
    "    for a in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        aus.append(a.text.strip())\n",
    "    #print(aus)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = aus + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_batswomen = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_batswomen.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI Batswomen in women ----->')\n",
    "    display(Top_10_ODI_batswomen)\n",
    "    print('\\n\\nand\\n\\n')\n",
    "    \n",
    "    #PART 3:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page3 = requests.get(url + '/rankings/womens/player-rankings/odi/bowling')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page3.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    jess = []\n",
    "    for j in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        jess.append(j.text)\n",
    "    #print(jess)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = jess + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    aus = []\n",
    "    for a in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        aus.append(a.text.strip())\n",
    "    #print(aus)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = aus + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_bowlingwomen = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_bowlingwomen.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI bolwingwomen in women ----->')\n",
    "    display(Top_10_ODI_bowlingwomen)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "        \n",
    "    #PART 4 :-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page4 = requests.get(url + '/rankings/womens/player-rankings/odi/all-rounder')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page4.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    ellyse = []\n",
    "    for e in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        ellyse.append(e.text)\n",
    "    #print(ellyse)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = ellyse + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    aus = []\n",
    "    for a in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        aus.append(a.text.strip())\n",
    "    #print(aus)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = aus + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_all_rounder = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_all_rounder.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI all rounder in women ----->')\n",
    "    display(Top_10_ODI_all_rounder)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "        \n",
    "    \n",
    "# Calling the function while specifying the URL\n",
    "function_6('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hdhdjfhjdhjd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e759119f57e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhdhdjfhjdhjd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'hdhdjfhjdhjd' is not defined"
     ]
    }
   ],
   "source": [
    "hdhdjfhjdhjd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7:-\n",
    "    Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-646f7e0b3614>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Calling the function while specifying the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mfunction_7\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.amazon.in/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-646f7e0b3614>\u001b[0m in \u001b[0;36mfunction_7\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m's?i=electronics&bbn=1389432031&rh=n%3A1389432031%2Cp_36%3A-2000000&dc&qid=1612616602&rnid=1318502031&ref=sr_nr_p_36_5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'apikey'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xxx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_7(url):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page = Request(url + 's?i=electronics&bbn=1389432031&rh=n%3A1389432031%2Cp_36%3A-2000000&dc&qid=1612616602&rnid=1318502031&ref=sr_nr_p_36_5')\n",
    "    page.add_header('apikey', 'xxx')\n",
    "    content = urlopen(page).read()\n",
    "    \n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('div', class_=\"s-main-slot s-result-list s-search-results sg-row\" )]\n",
    "    #print(table)\n",
    "\n",
    "    # Scrapping the name and specifications\n",
    "    mobile = [m.text.strip() for m in bs.find_all('a', class_='a-link-normal a-text-normal')]\n",
    "    print(mobile)\n",
    "    \n",
    "    # Scrapping the price\n",
    "    price = [p.text for p in bs.find_all('span', class_='a-price-whole')]\n",
    "    print(price)  \n",
    "    \n",
    "    # Scrapping the image urls\n",
    "    img = [i.get('src') for b in bs.find_all('a', class_='a-link-normal s-no-outline') for i in b.find_all('img')]\n",
    "    print(img)\n",
    "    \n",
    "    # Scrapping the ratings(stars)\n",
    "    stars = [s.text.strip() for b in bs.find_all('div', class_='a-row a-size-small') for s in b.find_all('span', class_='a-icon-alt')]\n",
    "    print(stars)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Calling the function while specifying the URL\n",
    "function_7('https://www.amazon.in/')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code was blocking from scrapping the data at some point after executing several times, and so was rejecting the page request each time. \n",
    "In order to overcome this problem I used multiple request from different search engines to fetch the request of the page.\n",
    "The website sometimes displays the scraped the data or it blocks the data, only these two things are occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The request for fetching the page content of the page [85, 69, 59, 181, 47, 179, 25, 92, 153, 213, 76, 96, 31, 48, 62, 79, 185, 206, 127, 180, 135, 99, 95, 170, 88, 3, 37, 162, 11, 198, 28, 2, 71, 86, 130, 148, 74, 160, 51, 202, 138, 4, 208, 29, 43, 186, 201, 70, 188, 93, 178, 90, 173, 205, 91, 41, 116, 114, 149, 124, 56, 45, 42, 184, 64, 19, 112, 10, 107, 6, 119, 113, 46, 57, 15, 94, 192, 140, 81, 12, 44, 203, 109, 110, 60, 38, 53, 136, 24, 1, 13, 54, 14, 66, 49, 212, 217, 157, 133, 175, 143, 23, 77, 73, 218, 106, 102, 16, 9, 174, 40, 163, 18, 168, 82, 171, 176, 154, 215, 132, 190, 187, 120, 166, 172, 169, 146, 193, 196, 78, 36, 209, 72, 214, 167, 155, 144, 27, 118, 211, 20, 89, 122, 115, 121, 55, 80, 21, 194, 58, 177, 189, 33, 103, 126, 164, 152, 22, 34, 30, 7, 145, 123, 87, 207, 195, 50, 105, 165, 125, 128, 183, 200, 35, 142, 139, 147, 159, 17, 26, 65, 182, 63, 104, 83, 100, 158, 191, 216, 129, 210, 131, 8, 5, 134, 84, 150, 61, 156, 111, 197, 141, 67, 117, 151, 199, 204, 75, 32, 137, 97, 108, 98, 161, 101, 39, 52, 68] has been blocked by amazon\n",
      "The request for fetching the page content of the page [85, 69, 59, 181, 47, 179, 25, 92, 153, 213, 76, 96, 31, 48, 62, 79, 185, 206, 127, 180, 135, 99, 95, 170, 88, 3, 37, 162, 11, 198, 28, 2, 71, 86, 130, 148, 74, 160, 51, 202, 138, 4, 208, 29, 43, 186, 201, 70, 188, 93, 178, 90, 173, 205, 91, 41, 116, 114, 149, 124, 56, 45, 42, 184, 64, 19, 112, 10, 107, 6, 119, 113, 46, 57, 15, 94, 192, 140, 81, 12, 44, 203, 109, 110, 60, 38, 53, 136, 24, 1, 13, 54, 14, 66, 49, 212, 217, 157, 133, 175, 143, 23, 77, 73, 218, 106, 102, 16, 9, 174, 40, 163, 18, 168, 82, 171, 176, 154, 215, 132, 190, 187, 120, 166, 172, 169, 146, 193, 196, 78, 36, 209, 72, 214, 167, 155, 144, 27, 118, 211, 20, 89, 122, 115, 121, 55, 80, 21, 194, 58, 177, 189, 33, 103, 126, 164, 152, 22, 34, 30, 7, 145, 123, 87, 207, 195, 50, 105, 165, 125, 128, 183, 200, 35, 142, 139, 147, 159, 17, 26, 65, 182, 63, 104, 83, 100, 158, 191, 216, 129, 210, 131, 8, 5, 134, 84, 150, 61, 156, 111, 197, 141, 67, 117, 151, 199, 204, 75, 32, 137, 97, 108, 98, 161, 101, 39, 52, 68] has been blocked by amazon\n",
      "The request for fetching the page content of the page [85, 69, 59, 181, 47, 179, 25, 92, 153, 213, 76, 96, 31, 48, 62, 79, 185, 206, 127, 180, 135, 99, 95, 170, 88, 3, 37, 162, 11, 198, 28, 2, 71, 86, 130, 148, 74, 160, 51, 202, 138, 4, 208, 29, 43, 186, 201, 70, 188, 93, 178, 90, 173, 205, 91, 41, 116, 114, 149, 124, 56, 45, 42, 184, 64, 19, 112, 10, 107, 6, 119, 113, 46, 57, 15, 94, 192, 140, 81, 12, 44, 203, 109, 110, 60, 38, 53, 136, 24, 1, 13, 54, 14, 66, 49, 212, 217, 157, 133, 175, 143, 23, 77, 73, 218, 106, 102, 16, 9, 174, 40, 163, 18, 168, 82, 171, 176, 154, 215, 132, 190, 187, 120, 166, 172, 169, 146, 193, 196, 78, 36, 209, 72, 214, 167, 155, 144, 27, 118, 211, 20, 89, 122, 115, 121, 55, 80, 21, 194, 58, 177, 189, 33, 103, 126, 164, 152, 22, 34, 30, 7, 145, 123, 87, 207, 195, 50, 105, 165, 125, 128, 183, 200, 35, 142, 139, 147, 159, 17, 26, 65, 182, 63, 104, 83, 100, 158, 191, 216, 129, 210, 131, 8, 5, 134, 84, 150, 61, 156, 111, 197, 141, 67, 117, 151, 199, 204, 75, 32, 137, 97, 108, 98, 161, 101, 39, 52, 68] has been blocked by amazon\n",
      "The request for fetching the page content of the page [85, 69, 59, 181, 47, 179, 25, 92, 153, 213, 76, 96, 31, 48, 62, 79, 185, 206, 127, 180, 135, 99, 95, 170, 88, 3, 37, 162, 11, 198, 28, 2, 71, 86, 130, 148, 74, 160, 51, 202, 138, 4, 208, 29, 43, 186, 201, 70, 188, 93, 178, 90, 173, 205, 91, 41, 116, 114, 149, 124, 56, 45, 42, 184, 64, 19, 112, 10, 107, 6, 119, 113, 46, 57, 15, 94, 192, 140, 81, 12, 44, 203, 109, 110, 60, 38, 53, 136, 24, 1, 13, 54, 14, 66, 49, 212, 217, 157, 133, 175, 143, 23, 77, 73, 218, 106, 102, 16, 9, 174, 40, 163, 18, 168, 82, 171, 176, 154, 215, 132, 190, 187, 120, 166, 172, 169, 146, 193, 196, 78, 36, 209, 72, 214, 167, 155, 144, 27, 118, 211, 20, 89, 122, 115, 121, 55, 80, 21, 194, 58, 177, 189, 33, 103, 126, 164, 152, 22, 34, 30, 7, 145, 123, 87, 207, 195, 50, 105, 165, 125, 128, 183, 200, 35, 142, 139, 147, 159, 17, 26, 65, 182, 63, 104, 83, 100, 158, 191, 216, 129, 210, 131, 8, 5, 134, 84, 150, 61, 156, 111, 197, 141, 67, 117, 151, 199, 204, 75, 32, 137, 97, 108, 98, 161, 101, 39, 52, 68] has been blocked by amazon\n",
      "The request for fetching the page content of the page [85, 69, 59, 181, 47, 179, 25, 92, 153, 213, 76, 96, 31, 48, 62, 79, 185, 206, 127, 180, 135, 99, 95, 170, 88, 3, 37, 162, 11, 198, 28, 2, 71, 86, 130, 148, 74, 160, 51, 202, 138, 4, 208, 29, 43, 186, 201, 70, 188, 93, 178, 90, 173, 205, 91, 41, 116, 114, 149, 124, 56, 45, 42, 184, 64, 19, 112, 10, 107, 6, 119, 113, 46, 57, 15, 94, 192, 140, 81, 12, 44, 203, 109, 110, 60, 38, 53, 136, 24, 1, 13, 54, 14, 66, 49, 212, 217, 157, 133, 175, 143, 23, 77, 73, 218, 106, 102, 16, 9, 174, 40, 163, 18, 168, 82, 171, 176, 154, 215, 132, 190, 187, 120, 166, 172, 169, 146, 193, 196, 78, 36, 209, 72, 214, 167, 155, 144, 27, 118, 211, 20, 89, 122, 115, 121, 55, 80, 21, 194, 58, 177, 189, 33, 103, 126, 164, 152, 22, 34, 30, 7, 145, 123, 87, 207, 195, 50, 105, 165, 125, 128, 183, 200, 35, 142, 139, 147, 159, 17, 26, 65, 182, 63, 104, 83, 100, 158, 191, 216, 129, 210, 131, 8, 5, 134, 84, 150, 61, 156, 111, 197, 141, 67, 117, 151, 199, 204, 75, 32, 137, 97, 108, 98, 161, 101, 39, 52, 68] has been blocked by amazon\n",
      "The request for fetching the page content of the page [85, 69, 59, 181, 47, 179, 25, 92, 153, 213, 76, 96, 31, 48, 62, 79, 185, 206, 127, 180, 135, 99, 95, 170, 88, 3, 37, 162, 11, 198, 28, 2, 71, 86, 130, 148, 74, 160, 51, 202, 138, 4, 208, 29, 43, 186, 201, 70, 188, 93, 178, 90, 173, 205, 91, 41, 116, 114, 149, 124, 56, 45, 42, 184, 64, 19, 112, 10, 107, 6, 119, 113, 46, 57, 15, 94, 192, 140, 81, 12, 44, 203, 109, 110, 60, 38, 53, 136, 24, 1, 13, 54, 14, 66, 49, 212, 217, 157, 133, 175, 143, 23, 77, 73, 218, 106, 102, 16, 9, 174, 40, 163, 18, 168, 82, 171, 176, 154, 215, 132, 190, 187, 120, 166, 172, 169, 146, 193, 196, 78, 36, 209, 72, 214, 167, 155, 144, 27, 118, 211, 20, 89, 122, 115, 121, 55, 80, 21, 194, 58, 177, 189, 33, 103, 126, 164, 152, 22, 34, 30, 7, 145, 123, 87, 207, 195, 50, 105, 165, 125, 128, 183, 200, 35, 142, 139, 147, 159, 17, 26, 65, 182, 63, 104, 83, 100, 158, 191, 216, 129, 210, 131, 8, 5, 134, 84, 150, 61, 156, 111, 197, 141, 67, 117, 151, 199, 204, 75, 32, 137, 97, 108, 98, 161, 101, 39, 52, 68] has been blocked by amazon\n",
      "0\n",
      "21\n",
      "24\n",
      "21\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'apend'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0294b112476b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;31m# Calling the function while specifying the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m \u001b[0mfunction_7\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.amazon.in/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m219\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-0294b112476b>\u001b[0m in \u001b[0;36mfunction_7\u001b[1;34m(url, last_page)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'src'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m                 \u001b[0mfull_list_mobile_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'apend'"
     ]
    }
   ],
   "source": [
    "import random \n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_7(url,last_page):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    keys = ['Accept', 'Accept-Language', 'Referer', 'Sec-Fetch-Dest', 'Sec-Fetch-Site', 'Sec-Fetch-User', 'Upgrade-Insecure-Requests']\n",
    "    values = [\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "          \"en-GB,en;q=0.9,mr-IN;q=0.8,mr;q=0.7,hi-IN;q=0.6,hi;q=0.5,en-US;q=0.4\",\"https://www.scrapehero.com/\",\n",
    "          \"document\",\"cross-site\",\"?1\",\"1\"]\n",
    "    \n",
    "    # Now we will use a loop to navigate through the pages\n",
    "    #pick any random page number\n",
    "    ran_page = list(range(1,last_page))\n",
    "    random.shuffle(ran_page)\n",
    "    \n",
    "    # These are the lists which will contain all the relevant data\n",
    "    mobile_name = []\n",
    "    mobile_price = []\n",
    "    mobile_img = []\n",
    "    mobile_rating = []\n",
    "    full_list_mobile_name = []\n",
    "    full_list_mobile_price = []\n",
    "    full_list_mobile_img = []\n",
    "    full_list_mobile_rating = []\n",
    "   \n",
    "\n",
    "    # Writing a request to fetch the data from the web page \n",
    "    for page_no in ran_page:\n",
    "        if page_no == 1:\n",
    "            page = Request(url + '/s?i=electronics&bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031%2Cp_36%3A-2000000&dc&qid=1612600046&rnid=1318502031&ref=sr_nr_p_36_1')\n",
    "        else:\n",
    "            page = Request(url + '/s?i=electronics&bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031%2Cp_36%3A-2000000&dc&page=' + str(page_no) + '&qid=1612600046&rnid=1318502031&ref=sr_nr_p_36_1')\n",
    "    #print('page number ---->',ran_page,'\\n\\n')\n",
    "    \n",
    "        #pick any random key+value to get to the url and open it for fetching the data\n",
    "        ran = random.randint(0, len(keys)-1)\n",
    "        page.add_header(keys[ran], values[ran])\n",
    "    \n",
    "        toggle_switch = 'ON'\n",
    "        try:\n",
    "            page_content = urlopen(page).read()\n",
    "        except:\n",
    "            print('The request for fetching the page content of the page', ran_page, 'has been blocked by amazon')\n",
    "            toggle_switch = 'OFF'\n",
    "    \n",
    "        if toggle_switch == 'ON':\n",
    "            \n",
    "            # Let's add time delay to our code so that the site does not sense any acticvity, as it was blocking earlier\n",
    "            time.sleep(random.randint(3,10))\n",
    "        \n",
    "            # Parsing and getting access to the content of the requested page\n",
    "            bs = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "            # Scrapping the name and specifications\n",
    "            mobile = bs.find_all('h2', class_='a-size-mini a-spacing-none a-color-base s-line-clamp-2')\n",
    "            print(len(mobile))\n",
    "    \n",
    "            # Scrapping the price\n",
    "            price = bs.find_all('span', class_='a-price-whole')\n",
    "            print(len(price))  \n",
    "    \n",
    "            # Scrapping the image urls\n",
    "            img = bs.find_all('a', class_='a-link-normal s-no-outline')\n",
    "            print(len(img))\n",
    "    \n",
    "            # Scrapping the ratings(stars)\n",
    "            stars = bs.find_all('div', class_='a-row a-size-small') \n",
    "            print(len(stars))\n",
    "        \n",
    "            for m in mobile:\n",
    "                full_list_mobile_name.append(m.text.strip('\\n'))\n",
    "        \n",
    "            for p in price:\n",
    "                full_list_mobile_price.append(p.text)\n",
    "                \n",
    "            for i in img:\n",
    "                url = i.find('img').get('src')\n",
    "                full_list_mobile_img.apend(url)\n",
    "            \n",
    "            for s in stars:\n",
    "                rating = s.find('span', class_='a-icon-alt').text\n",
    "                full_list_mobile_rating.append(rating.strip('\\n'))\n",
    "    \n",
    "                if (len(price)==len(img)) and (len(img)==len(stars)):\n",
    "                    for m in mobile:\n",
    "                        mobile_name.append(m.text.strip('\\n'))\n",
    "        \n",
    "                    for p in price:\n",
    "                        mobile_price.append(p.text)\n",
    "                \n",
    "                    for i in img:\n",
    "                        url = img.find('img').get('src')\n",
    "                        mobile_img.apend(url)\n",
    "            \n",
    "                    for s in stars:\n",
    "                        rating = s.find('span', class_='a-icon-alt').text\n",
    "                        mobile_rating.append(rating.strip('\\n'))\n",
    "            \n",
    "        \n",
    "    #Building the data frame\n",
    "    amazon_phones = pd.DataFrame({\n",
    "        'mobile_name' : mobile_name,\n",
    "        'mobile_price' : mobile_price,\n",
    "        'mobile_img' : mobile_img,\n",
    "        'mobile_rating' : mobile_rating\n",
    "    })\n",
    "    \n",
    "    display(amazon_phones)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')  \n",
    "\n",
    "    \n",
    "# Calling the function while specifying the URL\n",
    "function_7('https://www.amazon.in/', 219)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8:-\n",
    "    Write a python program to extract information about the local weather from the National Weather Service website of USA, https://www.weather.gov/ for the city, San Francisco. You need to extract data about 7 day extended forecast display for the city. The data should include period, short description, temperature and description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Forecast for San Francisco CA ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>img</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overnight</td>\n",
       "      <td>newimages/medium/nbkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>Low: 45 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monday</td>\n",
       "      <td>newimages/medium/bkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>High: 58 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MondayNight</td>\n",
       "      <td>newimages/medium/nbkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>Low: 48 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>newimages/medium/bkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>High: 58 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TuesdayNight</td>\n",
       "      <td>newimages/medium/nbkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>Low: 47 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>newimages/medium/few.png</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>High: 60 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WednesdayNight</td>\n",
       "      <td>newimages/medium/nsct.png</td>\n",
       "      <td>Partly Cloudy</td>\n",
       "      <td>Low: 48 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>newimages/medium/ra.png</td>\n",
       "      <td>Rain Likely</td>\n",
       "      <td>High: 59 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ThursdayNight</td>\n",
       "      <td>newimages/medium/nra.png</td>\n",
       "      <td>Rain Likely</td>\n",
       "      <td>Low: 50 °F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           period                        img     short_desc         temp\n",
       "0       Overnight  newimages/medium/nbkn.png  Mostly Cloudy   Low: 45 °F\n",
       "1          Monday   newimages/medium/bkn.png  Mostly Cloudy  High: 58 °F\n",
       "2     MondayNight  newimages/medium/nbkn.png  Mostly Cloudy   Low: 48 °F\n",
       "3         Tuesday   newimages/medium/bkn.png  Mostly Cloudy  High: 58 °F\n",
       "4    TuesdayNight  newimages/medium/nbkn.png  Mostly Cloudy   Low: 47 °F\n",
       "5       Wednesday   newimages/medium/few.png          Sunny  High: 60 °F\n",
       "6  WednesdayNight  newimages/medium/nsct.png  Partly Cloudy   Low: 48 °F\n",
       "7        Thursday    newimages/medium/ra.png    Rain Likely  High: 59 °F\n",
       "8   ThursdayNight   newimages/medium/nra.png    Rain Likely   Low: 50 °F"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "NOTE :- The extended forecast was available for 3 to 4 days only in the above format on the whole website and so was not able to scrape the information for comming week(say 6 or 7 days), but there was some information available in the detailed forecast table present on the website, so got data scrapped from both the tables\n",
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Detailed Forecast for San Francisco CA ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>desc</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overnight</td>\n",
       "      <td>Mostly cloudy, with a low around 45. West sout...</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>low around 45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monday</td>\n",
       "      <td>Mostly cloudy, with a high near 58. Calm wind ...</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>high near 58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday Night</td>\n",
       "      <td>Mostly cloudy, with a low around 48. West wind...</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>low around 48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Mostly cloudy, with a high near 58. West wind ...</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>high near 58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuesday Night</td>\n",
       "      <td>Mostly cloudy, with a low around 47. West wind...</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>low around 47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Sunny, with a high near 60.</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>high near 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wednesday Night</td>\n",
       "      <td>Partly cloudy, with a low around 48.</td>\n",
       "      <td>Partly cloudy</td>\n",
       "      <td>low around 48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>Rain likely, mainly after 4pm.  Mostly cloudy,...</td>\n",
       "      <td>Rain likely, mainly after 4pm.  Mostly cloudy</td>\n",
       "      <td>high near 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thursday Night</td>\n",
       "      <td>Rain likely, mainly before 10pm.  Mostly cloud...</td>\n",
       "      <td>Rain likely, mainly before 10pm.  Mostly cloudy</td>\n",
       "      <td>low around 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Friday</td>\n",
       "      <td>A chance of rain, mainly between 10am and 4pm....</td>\n",
       "      <td>A chance of rain, mainly between 10am and 4pm....</td>\n",
       "      <td>high near 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Friday Night</td>\n",
       "      <td>A chance of rain.  Mostly cloudy, with a low a...</td>\n",
       "      <td>A chance of rain.  Mostly cloudy</td>\n",
       "      <td>low around 49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>Rain likely.  Mostly cloudy, with a high near 58.</td>\n",
       "      <td>Rain likely.  Mostly cloudy</td>\n",
       "      <td>high near 58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Saturday Night</td>\n",
       "      <td>A chance of rain.  Mostly cloudy, with a low a...</td>\n",
       "      <td>A chance of rain.  Mostly cloudy</td>\n",
       "      <td>low around 46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>A chance of rain.  Partly sunny, with a high n...</td>\n",
       "      <td>A chance of rain.  Partly sunny</td>\n",
       "      <td>high near 57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             period                                               desc  \\\n",
       "0         Overnight  Mostly cloudy, with a low around 45. West sout...   \n",
       "1            Monday  Mostly cloudy, with a high near 58. Calm wind ...   \n",
       "2      Monday Night  Mostly cloudy, with a low around 48. West wind...   \n",
       "3           Tuesday  Mostly cloudy, with a high near 58. West wind ...   \n",
       "4     Tuesday Night  Mostly cloudy, with a low around 47. West wind...   \n",
       "5         Wednesday                        Sunny, with a high near 60.   \n",
       "6   Wednesday Night               Partly cloudy, with a low around 48.   \n",
       "7          Thursday  Rain likely, mainly after 4pm.  Mostly cloudy,...   \n",
       "8    Thursday Night  Rain likely, mainly before 10pm.  Mostly cloud...   \n",
       "9            Friday  A chance of rain, mainly between 10am and 4pm....   \n",
       "10     Friday Night  A chance of rain.  Mostly cloudy, with a low a...   \n",
       "11         Saturday  Rain likely.  Mostly cloudy, with a high near 58.   \n",
       "12   Saturday Night  A chance of rain.  Mostly cloudy, with a low a...   \n",
       "13           Sunday  A chance of rain.  Partly sunny, with a high n...   \n",
       "\n",
       "                                           short_desc    temperature  \n",
       "0                                       Mostly cloudy  low around 45  \n",
       "1                                       Mostly cloudy   high near 58  \n",
       "2                                       Mostly cloudy  low around 48  \n",
       "3                                       Mostly cloudy   high near 58  \n",
       "4                                       Mostly cloudy  low around 47  \n",
       "5                                               Sunny   high near 60  \n",
       "6                                       Partly cloudy  low around 48  \n",
       "7       Rain likely, mainly after 4pm.  Mostly cloudy   high near 59  \n",
       "8     Rain likely, mainly before 10pm.  Mostly cloudy  low around 50  \n",
       "9   A chance of rain, mainly between 10am and 4pm....   high near 59  \n",
       "10                   A chance of rain.  Mostly cloudy  low around 49  \n",
       "11                        Rain likely.  Mostly cloudy   high near 58  \n",
       "12                   A chance of rain.  Mostly cloudy  low around 46  \n",
       "13                    A chance of rain.  Partly sunny   high near 57  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_8(url):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page = requests.get(url + 'MapClick.php?lon=-122.41323847509919&lat=37.77766187310338#.YB-ZNugzZPY')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # First we will scrape the data from the extended forecast present on the page and then we will scrape the detailed forecast info present on that page.\n",
    "    # In this way we will get the forecast for the coming week aswell\n",
    "    # Scrapping all the required data from the extended forecast text\n",
    "    table = [b.text for b in bs.find_all('div', id='seven-day-forecast-container')]\n",
    "    #print(table)\n",
    "    \n",
    "    # Scrapping the period \n",
    "    period = [p.text for b in bs.find_all('li', class_='forecast-tombstone') for p in b.find_all('p', class_='period-name')]\n",
    "    #print(period)\n",
    "    \n",
    "    # Scrapping the image url\n",
    "    img = [i.get('src') for b in bs.find_all('li', class_='forecast-tombstone') for d in b.find_all('div', class_='tombstone-container') for i in d.find_all('img')]\n",
    "    #print(img)\n",
    "    \n",
    "    # Scrapping the short description\n",
    "    short_desc = [sd.text  for sd in bs.find_all('p', class_='short-desc')]\n",
    "    #print(short_desc)\n",
    "    \n",
    "    # Scrapping the temperature\n",
    "    temp = [t.text for t in bs.find_all('p', class_='temp')]\n",
    "    #print(temp)\n",
    "    \n",
    "    #Building the data frame\n",
    "    extended_forecast = pd.DataFrame({\n",
    "        'period' : period,\n",
    "        'img' : img,\n",
    "        'short_desc' : short_desc,\n",
    "        'temp' : temp\n",
    "    })\n",
    "    \n",
    "    print('Extended Forecast for San Francisco CA ----->')\n",
    "    display(extended_forecast)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "    print('NOTE :- The extended forecast was available for 3 to 4 days only in the above format on the whole website and so was not able to scrape the information for comming week(say 6 or 7 days), but there was some information available in the detailed forecast table present on the website, so got data scrapped from both the tables')\n",
    "    print('-----------------------------------------------------------------','\\n\\n') \n",
    "    \n",
    "    # Scrapping all the required data from the detailed forecast\n",
    "    table = [b.text for b in bs.find_all('div', id='detailed-forecast', class_='panel panel-default')]\n",
    "    #print(table)\n",
    "    \n",
    "    # Scrapping the period\n",
    "    period = [p.text for b in bs.find_all('div', class_='col-sm-2 forecast-label') for p in b.find_all('b')]\n",
    "    #print(period)\n",
    "     \n",
    "    # Scrapping the description \n",
    "    desc = [d.text for d in bs.find_all('div', class_='col-sm-10 forecast-text')]\n",
    "    #print(desc)\n",
    "     \n",
    "    # Now we need to seperate the information from the description\n",
    "    # We will be using regex to do the task\n",
    "    # Extract the short description\n",
    "    import re\n",
    "    short_desc = []\n",
    "    short_description = []\n",
    "    for s in desc:\n",
    "        short_desc.append(re.findall('^.*?(?=with)',s))#'([a-zA-Z]+?)\\s*([a-zA-Z]+?),'\n",
    "    #print(short_desc)\n",
    "    #for list_to_str in short_desc:\n",
    "        #for a in list_to_str:\n",
    "            #temp = ' '.join([str(elem) for elem in a])\n",
    "            #print(temp)\n",
    "            #short_description.append(temp)\n",
    "    for list_to_str in short_desc:\n",
    "        for a in list_to_str:\n",
    "            strip = a.strip('[]')\n",
    "            strip = strip.strip(' ,')\n",
    "            #print(strip)\n",
    "            short_description.append(strip)\n",
    "    \n",
    "    # Extract temperature\n",
    "    temp_F = []\n",
    "    temperature = []\n",
    "    for t in desc:\n",
    "        temp_F.append(re.findall('with.+?\\S+?[0-9].', t))\n",
    "    #print(temp_F)\n",
    "    for temp_to_strip in temp_F:\n",
    "        for b in temp_to_strip:\n",
    "            strip = b.strip('with.')\n",
    "            strip = strip.strip('a ')\n",
    "            #print(strip)\n",
    "            temperature.append(strip)\n",
    "            \n",
    "    \n",
    "    #Building the data frame\n",
    "    detailed_forecast = pd.DataFrame({\n",
    "        'period' : period,\n",
    "        'desc' : desc,\n",
    "        'short_desc' : short_description,\n",
    "        'temperature' : temperature\n",
    "    })\n",
    "    \n",
    "    print('Detailed Forecast for San Francisco CA ----->')\n",
    "    display(detailed_forecast)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print('-----------------------------------------------------------------','\\n\\n') \n",
    "\n",
    "# Calling the function while specifying the URL\n",
    "function_8('https://forecast.weather.gov/')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
