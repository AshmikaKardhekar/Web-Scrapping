{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup - It will be used to parse the source code and to extract the required data from the parent structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests - It will be used to get request to the web page server to get the source code pf the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from requests) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ashmika kardhekar\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1:-\n",
    "    Write a python program to display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPLAY ALL THE HEADER TAGS \n",
      "\n",
      "\n",
      "<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">Main Page</h1> \n",
      "\n",
      "Text from the header tags :-\n",
      "Main Page \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "From today's featured article \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Did you know ... \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "In the news \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "On this day \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfl-h2\"><span id=\"From_today.27s_featured_list\"></span><span class=\"mw-headline\" id=\"From_today's_featured_list\">From today's featured list</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "From today's featured list \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Today's featured picture \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Other areas of Wikipedia \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Wikipedia's sister projects \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Wikipedia languages \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h2>Navigation menu</h2> \n",
      "\n",
      "Text from the header tags :-\n",
      "Navigation menu \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-personal-label\">\n",
      "<span>Personal tools</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Personal tools\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-namespaces-label\">\n",
      "<span>Namespaces</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Namespaces\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-variants-label\">\n",
      "<span>Variants</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Variants\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-views-label\">\n",
      "<span>Views</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Views\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-cactions-label\">\n",
      "<span>More</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "More\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Search\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-navigation-label\">\n",
      "<span>Navigation</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Navigation\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-interaction-label\">\n",
      "<span>Contribute</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Contribute\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-tb-label\">\n",
      "<span>Tools</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Tools\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-coll-print_export-label\">\n",
      "<span>Print/export</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Print/export\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-wikibase-otherprojects-label\">\n",
      "<span>In other projects</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "In other projects\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "<h3 id=\"p-lang-label\">\n",
      "<span>Languages</span>\n",
      "</h3> \n",
      "\n",
      "Text from the header tags :-\n",
      "\n",
      "Languages\n",
      " \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_1(url):\n",
    "    # Writing a request to fetch the data from the web page \n",
    "    page = requests.get(url)\n",
    "    # Parsing \n",
    "    bs = BeautifulSoup(page.content, 'html.parser')\n",
    "    # Finding all the header tags\n",
    "    header_tags = bs.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "    # Creating a loop to print the headers found in the html structure\n",
    "    print('DISPLAY ALL THE HEADER TAGS','\\n\\n')\n",
    "    for h in header_tags:\n",
    "        # Printing the header tags\n",
    "        print(h,'\\n')\n",
    "        # Printing the text contained in the header tags\n",
    "        print('Text from the header tags :-')\n",
    "        print(h.text,'\\n')\n",
    "        print('---------------------------------------------------------------------------------------------------------------')\n",
    "# Calling the function while specifying the URL\n",
    "function_1(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2:-\n",
    "    Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Release_Year</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Genre</th>\n",
       "      <th>IMDB_Rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>142 min</td>\n",
       "      <td>Drama</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>175 min</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>152 min</td>\n",
       "      <td>Action, Crime, Drama</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>202 min</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>96 min</td>\n",
       "      <td>Crime, Drama</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Eternal Sunshine of the Spotless Mind</td>\n",
       "      <td>(2004)</td>\n",
       "      <td>108 min</td>\n",
       "      <td>Drama, Romance, Sci-Fi</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Amélie</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>122 min</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Snatch</td>\n",
       "      <td>(2000)</td>\n",
       "      <td>104 min</td>\n",
       "      <td>Comedy, Crime</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Requiem for a Dream</td>\n",
       "      <td>(2000)</td>\n",
       "      <td>102 min</td>\n",
       "      <td>Drama</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>American Beauty</td>\n",
       "      <td>(1999)</td>\n",
       "      <td>122 min</td>\n",
       "      <td>Drama</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Movie Release_Year  Runtime  \\\n",
       "Rank                                                                \n",
       "1                  The Shawshank Redemption       (1994)  142 min   \n",
       "2                             The Godfather       (1972)  175 min   \n",
       "3                           The Dark Knight       (2008)  152 min   \n",
       "4                    The Godfather: Part II       (1974)  202 min   \n",
       "5                              12 Angry Men       (1957)   96 min   \n",
       "...                                     ...          ...      ...   \n",
       "96    Eternal Sunshine of the Spotless Mind       (2004)  108 min   \n",
       "97                                   Amélie       (2001)  122 min   \n",
       "98                                   Snatch       (2000)  104 min   \n",
       "99                      Requiem for a Dream       (2000)  102 min   \n",
       "100                         American Beauty       (1999)  122 min   \n",
       "\n",
       "                       Genre IMDB_Rating  \n",
       "Rank                                      \n",
       "1                      Drama         9.3  \n",
       "2               Crime, Drama         9.2  \n",
       "3       Action, Crime, Drama         9.0  \n",
       "4               Crime, Drama         9.0  \n",
       "5               Crime, Drama         9.0  \n",
       "...                      ...         ...  \n",
       "96    Drama, Romance, Sci-Fi         8.3  \n",
       "97           Comedy, Romance         8.3  \n",
       "98             Comedy, Crime         8.3  \n",
       "99                     Drama         8.3  \n",
       "100                    Drama         8.3  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_2(url):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    # Parsing\n",
    "    bs = BeautifulSoup(page.content, 'html.parser') \n",
    "    \n",
    "    # Find all the tags containing the movie names\n",
    "    movies = bs.find_all('h3', class_='lister-item-header')\n",
    "    \n",
    "    movie_title = [t.text for m in movies for t in m.find_all('a')]\n",
    "    \n",
    "    release_year = [y.text for m in movies for y in m.find_all('span', class_='lister-item-year text-muted unbold')]\n",
    "    \n",
    "    #parts = bs.find_all('p', class_='text_muted')\n",
    "    \n",
    "    #certificate = [c.text for p in parts for c in p.find_all('span', class_='certificate')]\n",
    "    \n",
    "    runtime = [r.text for p in bs.find_all('p', class_='text-muted') for r in p.find_all('span', class_='runtime')]\n",
    "    \n",
    "    genre = [g.text.strip() for p in bs.find_all('p',  class_='text-muted') for g in p.find_all('span', class_='genre')]\n",
    "    \n",
    "    imdb_rating = [i.text for b in bs.find_all('div', class_='inline-block ratings-imdb-rating') for i in b.find_all('strong')]\n",
    "    \n",
    "    #metascore = [s.text for b in bs.find_all('div', class_='inline-block ratings-metascore') for s in b.find_all('span', class_='metascore')] \n",
    "    \n",
    "    #votes = [v.text for b in bs.find_all('span', class_='text-muted') for v in b.find_all(name='nv')]\n",
    "    \n",
    "    Rank = range(1,101)\n",
    "    #Building the data frame\n",
    "    Top_100_movies = pd.DataFrame({\n",
    "        'Rank' : Rank,\n",
    "        'Movie' : movie_title,\n",
    "        'Release_Year' : release_year,\n",
    "        #'Certificate' : certificate,\n",
    "        'Runtime' : runtime,\n",
    "        'Genre' : genre,\n",
    "        'IMDB_Rating' : imdb_rating,\n",
    "        #'MetaScore' : metascore,\n",
    "        #'Votes' : votes,\n",
    "        #'Director' : director,\n",
    "        #'Stars' : stars\n",
    "    })\n",
    "    \n",
    "    Top_100_movies.set_index('Rank', inplace=True)\n",
    "    # Saving the DataFrame to CSV\n",
    "    Top_100_movies.to_csv('IMDB top 100 movies.csv')\n",
    "    \n",
    "    return(Top_100_movies)\n",
    "\n",
    "function_2(\"https://www.imdb.com/search/title/?count=100&groups=top_1000&sort=user_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:-\n",
    "    Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movies</th>\n",
       "      <th>Release_Year</th>\n",
       "      <th>IMDB_Rating</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>(2006)</td>\n",
       "      <td>8.2</td>\n",
       "      <td>167 min</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>(2009)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>170 min</td>\n",
       "      <td>Comedy, Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>(2007)</td>\n",
       "      <td>8.4</td>\n",
       "      <td>165 min</td>\n",
       "      <td>Drama, Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dil Chahta Hai</td>\n",
       "      <td>(2001)</td>\n",
       "      <td>8.1</td>\n",
       "      <td>183 min</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Swades: We, the People</td>\n",
       "      <td>(2004)</td>\n",
       "      <td>8.2</td>\n",
       "      <td>210 min</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Wake Up Sid</td>\n",
       "      <td>(2009)</td>\n",
       "      <td>7.6</td>\n",
       "      <td>138 min</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Rangeela</td>\n",
       "      <td>(1995)</td>\n",
       "      <td>7.5</td>\n",
       "      <td>142 min</td>\n",
       "      <td>Comedy, Drama, Musical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Shatranj Ke Khilari</td>\n",
       "      <td>(1977)</td>\n",
       "      <td>7.7</td>\n",
       "      <td>129 min</td>\n",
       "      <td>Comedy, Drama, History</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Pyaar Ka Punchnama</td>\n",
       "      <td>(2011)</td>\n",
       "      <td>7.7</td>\n",
       "      <td>149 min</td>\n",
       "      <td>Comedy, Drama, Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Ek Hasina Thi</td>\n",
       "      <td>(2004)</td>\n",
       "      <td>7.5</td>\n",
       "      <td>120 min</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Movies Release_Year IMDB_Rating  Runtime  \\\n",
       "Rank                                                             \n",
       "1            Rang De Basanti       (2006)         8.2  167 min   \n",
       "2                   3 Idiots       (2009)         8.4  170 min   \n",
       "3           Taare Zameen Par       (2007)         8.4  165 min   \n",
       "4             Dil Chahta Hai       (2001)         8.1  183 min   \n",
       "5     Swades: We, the People       (2004)         8.2  210 min   \n",
       "...                      ...          ...         ...      ...   \n",
       "96               Wake Up Sid       (2009)         7.6  138 min   \n",
       "97                  Rangeela       (1995)         7.5  142 min   \n",
       "98       Shatranj Ke Khilari       (1977)         7.7  129 min   \n",
       "99        Pyaar Ka Punchnama       (2011)         7.7  149 min   \n",
       "100            Ek Hasina Thi       (2004)         7.5  120 min   \n",
       "\n",
       "                       Genre  \n",
       "Rank                          \n",
       "1       Comedy, Crime, Drama  \n",
       "2              Comedy, Drama  \n",
       "3              Drama, Family  \n",
       "4     Comedy, Drama, Romance  \n",
       "5                      Drama  \n",
       "...                      ...  \n",
       "96    Comedy, Drama, Romance  \n",
       "97    Comedy, Drama, Musical  \n",
       "98    Comedy, Drama, History  \n",
       "99    Comedy, Drama, Romance  \n",
       "100    Crime, Drama, Mystery  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_3(url):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page = requests.get(url)\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page.content, 'html.parser') \n",
    "    \n",
    "    # Scrape all the titles\n",
    "    Movies = [m.text for h3 in bs.find_all('h3', class_='lister-item-header') for m in h3.find_all('a')]\n",
    "    \n",
    "    # Scrape all the release year\n",
    "    Release_Year = [y.text for h3 in bs.find_all('h3', class_='lister-item-header') for y in h3.find_all('span', class_='lister-item-year')]\n",
    "    \n",
    "    # Scrape the rating\n",
    "    IMDB_Rating = [i.text for d in bs.find_all('div', class_='ipl-rating-star small') for i in d.find_all('span', class_='ipl-rating-star__rating')]\n",
    "    \n",
    "    # Scrape all the runtime\n",
    "    Runtime = [r.text for p in bs.find_all('p', class_='text-muted text-small') for r in p.find_all('span', class_='runtime')]\n",
    "    \n",
    "    # Scrape all the genres\n",
    "    Genre = [g.text.strip() for p in bs.find_all('p', class_='text-muted text-small') for g in p.find_all('span', class_='genre')]\n",
    "    \n",
    "    Rank = range(1,101)\n",
    "    #Building the data frame\n",
    "    Top_100_Indian_movies = pd.DataFrame({\n",
    "        'Rank' : Rank,\n",
    "        'Movies' : Movies,\n",
    "        'Release_Year' : Release_Year,\n",
    "        'IMDB_Rating' : IMDB_Rating,\n",
    "        'Runtime' : Runtime,\n",
    "        'Genre' : Genre\n",
    "    })\n",
    "\n",
    "    Top_100_Indian_movies.set_index('Rank', inplace=True)\n",
    "    # Saving the DataFrame to CSV\n",
    "    Top_100_Indian_movies.to_csv('IMDB top 100 Indian movies.csv')\n",
    "    \n",
    "    return(Top_100_Indian_movies)\n",
    "\n",
    "function_3('https://www.imdb.com/list/ls009997493/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4:-\n",
    "    Write a python program to scrap book name, author name, genre and book review of any 5 books from ‘www.bookpage.com’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page number ----> 582 \n",
      "\n",
      "\n",
      "Any 5 books from the page:- \n",
      "\n",
      "\n",
      "1 ---->\n",
      "book_name : The Red House\n",
      "author_name: Mark Haddon\n",
      "genre: Fiction\n",
      "review: \n",
      "\n",
      "The family vacation has been the subject of many a comedic essay and Chevy Chase film, but in Mark Haddon’s new book, it gets both the literary and psychological treatment.The Red House, Haddon’s first adult novel since 2006’s A Spot of Bother, is undoubtedly the writer’s most ambitious undertaking to date. And yet, it also pays homage to his most acclaimed book, The Curious Incident of the Dog in the Night-Time: In both stories, Haddon taps into the brains of children while at the same time making the quotidian feel larger than life.His latest offering follows one family—husband Dominic, wife Angela, children Alex, Daisy and Benjy—on a trip to Wales to visit Angela’s brother Richard and new wife and stepdaughter. From the onset, nobody is particularly excited about this sojourn (Angela and Richard haven’t been close since the death of their mother) and the laborious way in which the clan goes through the motions of “quality time” is one of the book’s many uncomfortable delights.Still, more interesting is Haddon’s creative approach to interiority. In lieu of typical authorial omniscience, he jumps from character to character’s brain—often multiple times within a single page—in a way that is both frenetic and delightful. In other words, don’t get put off if you’re initially confused; you’ll eventually get used to the device and come to appreciate Haddon’s ability to illuminate an incident from multiple points of view.Indeed, one of the book’s many great truths is that each person is too wrapped up in his own tiny dramas to appreciate anyone else’s: Angela in the loss of a stillborn child, Alex in his nascent need to assert his masculinity, Daisy in her conflicted (if fervent) relationship with God, and so on. This can lead—as in the case of several failed kisses—to miscommunications only comprehensible from the outside in. And yet, as the trip’s small incidents play out, the characters do change and do learn to understand one another—particularly the teenagers, for whom Haddon clearly has great compassion.It’s also worth mentioning that Haddon is a supremely talented and perceptive writer with a great love of language and poetics. If The Curious Incident relied on simple diction and vocabulary, this book is the opposite—lyrically flowing from thought to thought, brain to brain, potential connection to potential connection.\n",
      "\n",
      "\n",
      "2 ---->\n",
      "book_name : Shadow of Night\n",
      "author_name: Deborah Harkness\n",
      "genre: Fiction\n",
      "review: \n",
      "\n",
      "With her first novel, A Discovery of Witches, Deborah Harkness experienced the kind of success few authors dare to dream of. At a time when there is no shortage of books devoted to creatures that go bump in the night, Harkness’ hefty tome—the first in a trilogy—managed to find its way into the hands of millions of readers. It introduced readers to Diana Bishop, a witch who denies her craft, and Matthew Clairmont, an austere geneticist who also happens to be a vampire, entrancing us with the irresistible—but forbidden—relationship that developed between the two. When last we saw Diana and Matthew, they were attempting to travel to the 16th century in an effort to preserve their lives and unlock the secrets surrounding Diana’s magic.Shadow of Night picks up right where the previous book left off, with Diana and Matthew touching down in Elizabethan England. Their hunt for a witch who can help Diana harness her powers and the mysterious Ashmole 782 volume carries them through the labyrinthine streets of London, as well as across the sea to France and Prague. Initially reveling in her ability to bend time, Diana soon learns that you can travel across the globe and even across centuries, but the troubles of your life will always find you. As Diana struggles to master the secrets of her craft, she must also confront the fact that Matthew has been keeping some earth-shattering secrets.From the very first pages of Shadow of Night it is evident that this novel is as much a love story about a bygone era as it is about Matthew and Diana. It overflows with a colorful cast of characters, many of whom Harkness has plucked straight from the history books, and Harkness renders the late 1500s in exquisite detail. At times, this meticulousness causes the plot to stall, but the writing is so rich, and the characters so compelling, readers are sure to forgive. Best of all, Harkness manages to execute with aplomb the act of answering old questions while posing new ones that will intensify anticipation for the final installment. Readers who have been counting down the days, take heart: The wait was most assuredly worth it.ALSO IN BOOKPAGE:Read a Q&A with Deborah Harkness for A Discovery of Witches.\n",
      "\n",
      "\n",
      "3 ---->\n",
      "book_name : Alif the Unseen\n",
      "author_name: G. Willow Wilson\n",
      "genre: Fiction\n",
      "review: \n",
      "\n",
      "“Sometimes anger is the pure and determined light that shows you the way forward.” With these introductory words, author G. Willow Wilson sends her ferocious and joyful debut novel into the world—more accurately, her debut text-based novel, for Alif the Unseen comes on the winged heels of her award-winning graphic work. As with every comic-book artist turned author, the critical question is this: Can her talent for vivid characterization translate from image into text?\n",
      "The answer, in Wilson’s case, is a resounding “yes.” The main reason for her success is her anger. It’s a courageous matter for a Muslim woman to express rage to a Western readership that has come so readily to equate Muslim rage with the horrors of 9/11. But a decade after that catastrophe, the fragile Arab Spring has given a new face to Muslim anger: not a murderous hatred, but a fine outrage against tyranny; a gutsy and spontaneous resistance against various regimes who have falsely equated freedom with apostasy.\n",
      "Computer technology, and the social networking it has engendered, has been the most powerful tool for this painful process of liberation. It is inevitable, then, for the hero of Alif the Unseen to be a computer hacker, that most “unseen” of antiheroes. Alif only wanted to use his geeky skills to be with Intisar, his unattainable beloved. Instead, he finds himself at the head of a vast political storm—at first inadvertently, but then with whole heart. He is a modern-day Aladdin who rubs not a magic lamp, but a mouse pad, thus loosing chaos into his unnamed Arabian city, including genie (properly spelled “jinn”), demons and “The Hand,” who seeks to control the fate of his people.\n",
      "At the center of the tale stands the figure of Dina, the “unbeautiful” maiden who hides her light under a bushel of Muslim veils. Dina points Alif to the redemption of his world through love—the unseen first principle of every great faith tradition.\n",
      "Whatever the critical response may be to Wilson’s unique and unruly literary gifts, there is no question that Alif the Unseen is one of those rare events in the history of publishing, when an ancient pattern of storytelling (The Arabian Nights) is grafted onto an up-to-the-minute world crisis. This synthesis has great spiritual authority, thanks to the vision of G. Willow Wilson.\n",
      "\n",
      "\n",
      "4 ---->\n",
      "book_name : Coming to My Senses\n",
      "author_name: Alyssa Harad\n",
      "genre: Historical Fiction\n",
      "review: \n",
      "\n",
      "Woodsy and seductive, with a hint of spice, Coming to My Senses: A Story of Perfume, Pleasure, and an Unlikely Bride offers a luscious immersion in the world of perfume obsession. But what makes this memoir so appealing are its deeper notes, the ones that linger on after reading: the story of a how a no-nonsense, underemployed English Ph.D., who usually dresses like “an unmade bed,” discovers the pleasures of femininity and her own senses through an affair with fragrance.Stumbling onto the world of perfume blogs late one night, Alyssa Harad discovers a new and fascinating world of scent and language; she is as seduced by the perfumes as by the challenge of describing them. Samples start arriving in the mail, and Harad begins to develop a “vocabulary of scents” to describe the “scratchy, dirty richness” of patchouli or the “drugged, dreamy” sensation of jasmine. One afternoon, a magical transformation occurs: A honeyed wine fragrance inspires her to dump the sweats, and put on earrings and lipstick—the freelance writer as Cinderella!The elegance of Harad’s narrative comes as much from what it doesn’t say as what it does. Unlike many contemporary memoirs, Coming to My Senses contains no trauma, no bad childhood and no exposé of her relationship with boyfriend V. (she mentions postponing their wedding, but we never learn exactly why). Such reticence is refreshing, even ladylike, and after all, there is so much to say about the scents of saffron and vetiver, the “aunties” back home in Boise and the unexpected kindness of Bergdorf’s salespeople. We never miss the trauma. In fact, this memoir performs a kind of inspirational function: I’m wearing a blend of gardenia and cherry blossom as I write this. Now if I could just get out of these yoga pants.\n",
      "\n",
      "\n",
      "5 ---->\n",
      "book_name : Yes, Chef\n",
      "author_name: Marcus Samuelsson\n",
      "genre: Debut Fiction\n",
      "review: \n",
      "\n",
      "Marcus Samuelsson made his name as one of the youngest executive chefs in Manhattan and a familiar face on the Food Network.What might be less familiar is Samuelsson’s fascinating personal history, which he lays bare in Yes, Chef. Born in Ethiopia, Samuelsson and his sister became dangerously ill with tuberculosis. Their mother walked with them from their remote village to Addis Ababa, where she died. The children were adopted by a loving Swedish family.Samuelsson spent much of his childhood at the elbow of his Swedish grandmother, an excellent home cook, and went on to work at restaurants in Europe. But after a horrific car accident killed one of his closest friends, Samuelsson sought an apprenticeship to take him away from his grief. He landed at New York’s Aquavit, a restaurant that is, he writes, “more Swedish in its menu than any I had ever worked in.”This was the beginning of a love affair with New York City. To read his descriptions of the food he eats, from steamed buns in Chinatown to roasted meats from street vendors, is to almost viscerally experience the smells, sounds and sights of the city.Although he traveled the world learning about every cuisine from French to Mexican, Samuelsson was at a loss when a student asked him to describe trends in African cooking. He had not set foot on the continent since he was a toddler. Rediscovering his Ethiopian roots led him to open the successful Red Rooster in 2010, deliberately choosing the underappreciated streets of Harlem as the site for the restaurant.Samuelsson’s is the most unlikely of journeys, and he takes readers along every step of the way in this delicious memoir.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_4(url):\n",
    "    \n",
    "    #pick any random page number\n",
    "    ran = random.randint(1,1000)\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page \n",
    "    if ran == 1:\n",
    "        page = requests.get(url + '/reviews')\n",
    "    else:\n",
    "        page = requests.get(url + '/reviews?page=' + str(ran))\n",
    "    print('page number ---->',ran,'\\n\\n')\n",
    "    \n",
    "    # Parsing \n",
    "    bs = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Finding all book names and storing them into a list\n",
    "    book_name = [n.text for b in bs.find_all('h4', class_='italic') for n in b.find_all('a')]\n",
    "    #print(book_name)\n",
    "    \n",
    "    # Finding all author names and storing them into a list\n",
    "    author_name = [a.text.strip('\\n') for a in bs.find_all('p', class_='sans bold')]\n",
    "    #print(author_name)\n",
    "    \n",
    "    # Finding all genres and storing them into a list\n",
    "    genre = [g.text.strip() for i in bs.find_all('p', class_='genre-links hidden-phone') for g in i.find_all('a')]\n",
    "    #print(genre)\n",
    "    \n",
    "    # Finding all reviews along with their urls and storing them into a list\n",
    "    reviews = [r.get('href') for b in bs.find_all('div', class_='read-full') for r in b.find_all('a')]\n",
    "    #print(reviews)\n",
    "    \n",
    "    # We need only the body of the article, so creating a loop for getting full reviews\n",
    "    review = []\n",
    "    for r in reviews:\n",
    "        review_page = requests.get('https://www.bookpage.com' + r)\n",
    "        soup = BeautifulSoup(review_page.content,'html.parser')\n",
    "        read_reviews = soup.find_all('div', class_='article-body')\n",
    "        for rr in read_reviews:\n",
    "            review.append(rr.text)\n",
    "            \n",
    "    #print(review)\n",
    "    \n",
    "    #Printing any 5 random books\n",
    "    print('Any 5 books from the page:-','\\n\\n')\n",
    "\n",
    "    # Choosing any 5 random books from the selected random page\n",
    "    ran_index = set()\n",
    "    while len(ran_index) < 5:\n",
    "        ran_index.add(random.randint(0,9))\n",
    "        \n",
    "    for i,index in enumerate(ran_index,start=1):\n",
    "        print(i,'---->')\n",
    "        print('book_name :', book_name[index])\n",
    "        print('author_name:', author_name[index])\n",
    "        print('genre:', genre[index])\n",
    "        try:\n",
    "            print('review:', review[index])\n",
    "        except:\n",
    "            print('review:', 'NOT AVAILABLE')\n",
    "        \n",
    "# Calling the function while specifying the URL\n",
    "function_4('http://www.bookpage.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5:-\n",
    "    Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:                               \n",
    "    i)   Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.                                   ii)  Top 10 ODI Batsmen in men along with the records of their team and rating.    \n",
    "    iii) Top 10 ODI bowlers along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in men’s cricket ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>matches</th>\n",
       "      <th>points</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENG</td>\n",
       "      <td>44</td>\n",
       "      <td>5,405</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IND</td>\n",
       "      <td>52</td>\n",
       "      <td>6,102</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NZ</td>\n",
       "      <td>32</td>\n",
       "      <td>3,716</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUS</td>\n",
       "      <td>39</td>\n",
       "      <td>4,344</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SA</td>\n",
       "      <td>31</td>\n",
       "      <td>3,345</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PAK</td>\n",
       "      <td>35</td>\n",
       "      <td>3,490</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BAN</td>\n",
       "      <td>37</td>\n",
       "      <td>3,366</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SL</td>\n",
       "      <td>39</td>\n",
       "      <td>3,297</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WI</td>\n",
       "      <td>46</td>\n",
       "      <td>3,402</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AFG</td>\n",
       "      <td>31</td>\n",
       "      <td>1,844</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     team matches points rating\n",
       "rank                           \n",
       "1     ENG      44  5,405    123\n",
       "2     IND      52  6,102    117\n",
       "3      NZ      32  3,716    116\n",
       "4     AUS      39  4,344    111\n",
       "5      SA      31  3,345    108\n",
       "6     PAK      35  3,490    100\n",
       "7     BAN      37  3,366     91\n",
       "8      SL      39  3,297     85\n",
       "9      WI      46  3,402     74\n",
       "10    AFG      31  1,844     59"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Top 10 ODI Batsmen in men ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>870</td>\n",
       "      <td>911 v England, 12/07/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>842</td>\n",
       "      <td>885 v Sri Lanka, 06/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>837</td>\n",
       "      <td>846 v Sri Lanka, 20/10/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>818</td>\n",
       "      <td>841 v Bangladesh, 05/06/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aaron Finch</td>\n",
       "      <td>AUS</td>\n",
       "      <td>791</td>\n",
       "      <td>798 v England, 25/06/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Francois du Plessis</td>\n",
       "      <td>SA</td>\n",
       "      <td>790</td>\n",
       "      <td>820 v Australia, 06/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>773</td>\n",
       "      <td>880 v Pakistan, 26/01/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kane Williamson</td>\n",
       "      <td>NZ</td>\n",
       "      <td>765</td>\n",
       "      <td>799 v India, 09/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>755</td>\n",
       "      <td>813 v Sri Lanka, 10/03/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>754</td>\n",
       "      <td>777 v Australia, 21/06/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   player team rating            career_best_rating\n",
       "rank                                                               \n",
       "1             Virat Kohli  IND    870     911 v England, 12/07/2018\n",
       "2            Rohit Sharma  IND    842   885 v Sri Lanka, 06/07/2019\n",
       "3              Babar Azam  PAK    837   846 v Sri Lanka, 20/10/2017\n",
       "4             Ross Taylor   NZ    818  841 v Bangladesh, 05/06/2019\n",
       "5             Aaron Finch  AUS    791     798 v England, 25/06/2019\n",
       "6     Francois du Plessis   SA    790   820 v Australia, 06/07/2019\n",
       "7            David Warner  AUS    773    880 v Pakistan, 26/01/2017\n",
       "8         Kane Williamson   NZ    765       799 v India, 09/07/2019\n",
       "9         Quinton de Kock   SA    755   813 v Sri Lanka, 10/03/2019\n",
       "10         Jonny Bairstow  ENG    754   777 v Australia, 21/06/2018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Top 10 ODI bolwingmen in men ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>722</td>\n",
       "      <td>770 v West Indies, 22/06/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>708</td>\n",
       "      <td>712 v Ireland, 24/01/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>700</td>\n",
       "      <td>841 v West Indies, 01/11/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>694</td>\n",
       "      <td>694 v West Indies, 25/01/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chris Woakes</td>\n",
       "      <td>ENG</td>\n",
       "      <td>675</td>\n",
       "      <td>676 v New Zealand, 14/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kagiso Rabada</td>\n",
       "      <td>SA</td>\n",
       "      <td>665</td>\n",
       "      <td>724 v England, 29/05/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>660</td>\n",
       "      <td>733 v England, 26/01/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mustafizur Rahman</td>\n",
       "      <td>BAN</td>\n",
       "      <td>658</td>\n",
       "      <td>695 v West Indies, 14/12/2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mohammad Amir</td>\n",
       "      <td>PAK</td>\n",
       "      <td>647</td>\n",
       "      <td>663 v Sri Lanka, 02/10/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pat Cummins</td>\n",
       "      <td>AUS</td>\n",
       "      <td>646</td>\n",
       "      <td>729 v Pakistan, 12/06/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 player team rating             career_best_rating\n",
       "rank                                                              \n",
       "1           Trent Boult   NZ    722  770 v West Indies, 22/06/2019\n",
       "2      Mujeeb Ur Rahman  AFG    708      712 v Ireland, 24/01/2021\n",
       "3        Jasprit Bumrah  IND    700  841 v West Indies, 01/11/2018\n",
       "4          Mehedi Hasan  BAN    694  694 v West Indies, 25/01/2021\n",
       "5          Chris Woakes  ENG    675  676 v New Zealand, 14/07/2019\n",
       "6         Kagiso Rabada   SA    665      724 v England, 29/05/2017\n",
       "7        Josh Hazlewood  AUS    660      733 v England, 26/01/2018\n",
       "8     Mustafizur Rahman  BAN    658  695 v West Indies, 14/12/2018\n",
       "9         Mohammad Amir  PAK    647    663 v Sri Lanka, 02/10/2019\n",
       "10          Pat Cummins  AUS    646     729 v Pakistan, 12/06/2019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_5(url):\n",
    "    \n",
    "    #PART 1:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page1 = requests.get(url + '/rankings/mens/team-rankings/odi')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page1.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "    \n",
    "    # Scraping Country names\n",
    "    team = [t.text for b in bs.find_all('tbody') for t in b.find_all('span', class_='u-show-phablet')]\n",
    "    #print(team)\n",
    "    \n",
    "    # Scraping the Matches\n",
    "    matches_england = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for me in b.find_all('td', class_='rankings-block__banner--matches'):\n",
    "            matches_england.append(me.text)\n",
    "    #print(matches_england)\n",
    "    \n",
    "    matches = [m.text for b in bs.find_all('tr', class_='table-body') for m in b.find_all('td', class_='table-body__cell u-center-text')]\n",
    "    matches = matches[0::2]\n",
    "    matches = matches_england  + matches\n",
    "    #print(matches)\n",
    "    \n",
    "    # Scrapping the points\n",
    "    points_england = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for pe in b.find_all('td', class_='rankings-block__banner--points'):\n",
    "            points_england.append(pe.text)\n",
    "    #print(points_england)\n",
    "    \n",
    "    points = [p.text for b in bs.find_all('tr', class_='table-body') for p in b.find_all('td', class_='table-body__cell u-center-text')]\n",
    "    points = points[1::2]\n",
    "    points = points_england + points\n",
    "    #print(points)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rating_england = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for re in b.find_all('td', class_='rankings-block__banner--rating u-text-right'):\n",
    "            rating_england.append(re.text.strip())\n",
    "    #print(rating_england)\n",
    "    \n",
    "    rating = [r.text for b in bs.find_all('tr', class_='table-body') for r in b.find_all('td', class_='table-body__cell u-text-right rating')]\n",
    "    rating = rating_england + rating\n",
    "    #print(rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_men = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'team' : team[0:10],\n",
    "        'matches' : matches[0:10],\n",
    "        'points' : points[0:10],\n",
    "        'rating' : rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_men.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI teams in men’s cricket ----->')\n",
    "    display(Top_10_ODI_men)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "    \n",
    "    #PART 2:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page2 = requests.get(url + '/rankings/mens/player-rankings/odi/batting')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page2.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    kohli = []\n",
    "    for k in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        kohli.append(k.text)\n",
    "    #print(kohli)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = kohli + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    india = []\n",
    "    for i in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        india.append(i.text.strip())\n",
    "    #print(india)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = india + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_batsmen = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_batsmen.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI Batsmen in men ----->')\n",
    "    display(Top_10_ODI_batsmen)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "    \n",
    "    #PART 3:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page3 = requests.get(url + '/rankings/mens/player-rankings/odi/bowling')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page3.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    trent = []\n",
    "    for tb in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        trent.append(tb.text)\n",
    "    #print(trent)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = trent + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    nz = []\n",
    "    for n in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        nz.append(n.text.strip())\n",
    "    #print(nz)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = nz + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_bowlingmen = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_bowlingmen.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI bolwingmen in men ----->')\n",
    "    display(Top_10_ODI_bowlingmen)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "        \n",
    "# Calling the function while specifying the URL\n",
    "function_5('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6:-\n",
    "    Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:\n",
    "       i)   Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "       ii)  Top 10 women’s ODI players along with the records of their team and rating.\n",
    "       iii) Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in women’s cricket ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>matches</th>\n",
       "      <th>points</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUS</td>\n",
       "      <td>15</td>\n",
       "      <td>2,436</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IND</td>\n",
       "      <td>15</td>\n",
       "      <td>1,812</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENG</td>\n",
       "      <td>14</td>\n",
       "      <td>1,670</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SA</td>\n",
       "      <td>19</td>\n",
       "      <td>2,090</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NZ</td>\n",
       "      <td>15</td>\n",
       "      <td>1,384</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WI</td>\n",
       "      <td>12</td>\n",
       "      <td>1,025</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PAK</td>\n",
       "      <td>15</td>\n",
       "      <td>1,101</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BAN</td>\n",
       "      <td>5</td>\n",
       "      <td>306</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SL</td>\n",
       "      <td>11</td>\n",
       "      <td>519</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>IRE</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     team matches points rating\n",
       "rank                           \n",
       "1     AUS      15  2,436    162\n",
       "2     IND      15  1,812    121\n",
       "3     ENG      14  1,670    119\n",
       "4      SA      19  2,090    110\n",
       "5      NZ      15  1,384     92\n",
       "6      WI      12  1,025     85\n",
       "7     PAK      15  1,101     73\n",
       "8     BAN       5    306     61\n",
       "9      SL      11    519     47\n",
       "10    IRE       2     25     13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Top 10 ODI Batswomen in women ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "      <td>834 v New Zealand, 24/02/2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>746</td>\n",
       "      <td>765 v India, 02/03/2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>741</td>\n",
       "      <td>741 v New Zealand, 07/10/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>732</td>\n",
       "      <td>797 v England, 28/02/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>723</td>\n",
       "      <td>756 v Australia, 02/03/2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tammy Beaumont</td>\n",
       "      <td>ENG</td>\n",
       "      <td>716</td>\n",
       "      <td>738 v Australia, 04/07/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>691</td>\n",
       "      <td>712 v New Zealand, 25/01/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>691</td>\n",
       "      <td>766 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mithali Raj</td>\n",
       "      <td>IND</td>\n",
       "      <td>687</td>\n",
       "      <td>839 v Australia, 24/12/2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Lizelle Lee</td>\n",
       "      <td>SA</td>\n",
       "      <td>681</td>\n",
       "      <td>752 v England, 12/06/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 player team rating             career_best_rating\n",
       "rank                                                              \n",
       "1           Meg Lanning  AUS    749  834 v New Zealand, 24/02/2016\n",
       "2       Stafanie Taylor   WI    746        765 v India, 02/03/2012\n",
       "3          Alyssa Healy  AUS    741  741 v New Zealand, 07/10/2020\n",
       "4       Smriti Mandhana  IND    732      797 v England, 28/02/2019\n",
       "5     Amy Satterthwaite   NZ    723    756 v Australia, 02/03/2017\n",
       "6        Tammy Beaumont  ENG    716    738 v Australia, 04/07/2019\n",
       "7       Laura Wolvaardt   SA    691  712 v New Zealand, 25/01/2020\n",
       "8          Ellyse Perry  AUS    691  766 v West Indies, 11/09/2019\n",
       "9           Mithali Raj  IND    687    839 v Australia, 24/12/2004\n",
       "10          Lizelle Lee   SA    681      752 v England, 12/06/2018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "and\n",
      "\n",
      "\n",
      "Top 10 ODI bolwingwomen in women ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>804</td>\n",
       "      <td>804 v New Zealand, 07/10/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megan Schutt</td>\n",
       "      <td>AUS</td>\n",
       "      <td>735</td>\n",
       "      <td>766 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>711</td>\n",
       "      <td>738 v New Zealand, 30/01/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shabnim Ismail</td>\n",
       "      <td>SA</td>\n",
       "      <td>708</td>\n",
       "      <td>717 v New Zealand, 30/01/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>691</td>\n",
       "      <td>796 v England, 28/02/2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Poonam Yadav</td>\n",
       "      <td>IND</td>\n",
       "      <td>679</td>\n",
       "      <td>680 v West Indies, 06/11/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shikha Pandey</td>\n",
       "      <td>IND</td>\n",
       "      <td>675</td>\n",
       "      <td>696 v South Africa, 11/10/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>666</td>\n",
       "      <td>719 v West Indies, 05/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Anya Shrubsole</td>\n",
       "      <td>ENG</td>\n",
       "      <td>645</td>\n",
       "      <td>655 v Pakistan, 12/12/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>639</td>\n",
       "      <td>650 v England, 22/02/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              player team rating              career_best_rating\n",
       "rank                                                            \n",
       "1      Jess Jonassen  AUS    804   804 v New Zealand, 07/10/2020\n",
       "2       Megan Schutt  AUS    735   766 v West Indies, 11/09/2019\n",
       "3     Marizanne Kapp   SA    711   738 v New Zealand, 30/01/2020\n",
       "4     Shabnim Ismail   SA    708   717 v New Zealand, 30/01/2020\n",
       "5     Jhulan Goswami  IND    691       796 v England, 28/02/2007\n",
       "6       Poonam Yadav  IND    679   680 v West Indies, 06/11/2019\n",
       "7      Shikha Pandey  IND    675  696 v South Africa, 11/10/2019\n",
       "8       Ellyse Perry  AUS    666   719 v West Indies, 05/09/2019\n",
       "9     Anya Shrubsole  ENG    645      655 v Pakistan, 12/12/2019\n",
       "10     Deepti Sharma  IND    639       650 v England, 22/02/2019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Top 10 ODI all rounder in women ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>team</th>\n",
       "      <th>rating</th>\n",
       "      <th>career_best_rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>460</td>\n",
       "      <td>548 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stafanie Taylor</td>\n",
       "      <td>WI</td>\n",
       "      <td>410</td>\n",
       "      <td>559 v New Zealand, 10/10/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>396</td>\n",
       "      <td>412 v Pakistan, 23/01/2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>359</td>\n",
       "      <td>397 v South Africa, 09/10/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>301</td>\n",
       "      <td>308 v West Indies, 11/09/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dane van Niekerk</td>\n",
       "      <td>SA</td>\n",
       "      <td>297</td>\n",
       "      <td>421 v Sri Lanka, 11/02/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sophie Devine</td>\n",
       "      <td>NZ</td>\n",
       "      <td>289</td>\n",
       "      <td>305 v Australia, 05/10/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>273</td>\n",
       "      <td>307 v Sri Lanka, 21/03/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shikha Pandey</td>\n",
       "      <td>IND</td>\n",
       "      <td>250</td>\n",
       "      <td>262 v South Africa, 14/10/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Katherine Brunt</td>\n",
       "      <td>ENG</td>\n",
       "      <td>232</td>\n",
       "      <td>270 v Sri Lanka, 16/03/2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                player team rating              career_best_rating\n",
       "rank                                                              \n",
       "1         Ellyse Perry  AUS    460   548 v West Indies, 11/09/2019\n",
       "2      Stafanie Taylor   WI    410   559 v New Zealand, 10/10/2013\n",
       "3       Marizanne Kapp   SA    396      412 v Pakistan, 23/01/2021\n",
       "4        Deepti Sharma  IND    359  397 v South Africa, 09/10/2019\n",
       "5        Jess Jonassen  AUS    301   308 v West Indies, 11/09/2019\n",
       "6     Dane van Niekerk   SA    297     421 v Sri Lanka, 11/02/2019\n",
       "7        Sophie Devine   NZ    289     305 v Australia, 05/10/2020\n",
       "8       Natalie Sciver  ENG    273     307 v Sri Lanka, 21/03/2019\n",
       "9        Shikha Pandey  IND    250  262 v South Africa, 14/10/2019\n",
       "10     Katherine Brunt  ENG    232     270 v Sri Lanka, 16/03/2019"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_6(url):\n",
    "    \n",
    "    #PART 1:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page1 = requests.get(url + '/rankings/womens/team-rankings/odi')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page1.content, 'html.parser')\n",
    "    \n",
    "    # Scrapping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "    \n",
    "     # Scraping Country names\n",
    "    team = [t.text for b in bs.find_all('tbody') for t in b.find_all('span', class_='u-show-phablet')]\n",
    "    #print(team)\n",
    "    \n",
    "    # Scraping the Matches\n",
    "    matches_australia = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for ma in b.find_all('td', class_='rankings-block__banner--matches'):\n",
    "            matches_australia.append(ma.text)\n",
    "    #print(matches_australia)\n",
    "    \n",
    "    matches = [m.text for b in bs.find_all('tr', class_='table-body') for m in b.find_all('td', class_='table-body__cell u-center-text')]\n",
    "    matches = matches[0::2]\n",
    "    matches = matches_australia  + matches\n",
    "    #print(matches)\n",
    "    \n",
    "    # Scrapping the points\n",
    "    points_australia = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for pa in b.find_all('td', class_='rankings-block__banner--points'):\n",
    "            points_australia.append(pa.text)\n",
    "    #print(points_england)\n",
    "    \n",
    "    points = [p.text for b in bs.find_all('tr', class_='table-body') for p in b.find_all('td', class_='table-body__cell u-center-text')]\n",
    "    points = points[1::2]\n",
    "    points = points_australia + points\n",
    "    #print(points)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rating_australia = []\n",
    "    for b in bs.find_all('tbody'):\n",
    "        for ra in b.find_all('td', class_='rankings-block__banner--rating u-text-right'):\n",
    "            rating_australia.append(ra.text.strip())\n",
    "    #print(rating_australia)\n",
    "    \n",
    "    rating = [r.text for b in bs.find_all('tr', class_='table-body') for r in b.find_all('td', class_='table-body__cell u-text-right rating')]\n",
    "    rating = rating_australia + rating\n",
    "    #print(rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_women = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'team' : team[0:10],\n",
    "        'matches' : matches[0:10],\n",
    "        'points' : points[0:10],\n",
    "        'rating' : rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_women.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI teams in women’s cricket ----->')\n",
    "    display(Top_10_ODI_women)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "    \n",
    "    #PART 2:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page2 = requests.get(url + '/rankings/womens/player-rankings/odi/batting')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page2.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    meg = []\n",
    "    for me in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        meg.append(me.text)\n",
    "    #print(meg)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = meg + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    aus = []\n",
    "    for a in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        aus.append(a.text.strip())\n",
    "    #print(aus)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = aus + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_batswomen = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_batswomen.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI Batswomen in women ----->')\n",
    "    display(Top_10_ODI_batswomen)\n",
    "    print('\\n\\nand\\n\\n')\n",
    "    \n",
    "    #PART 3:-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page3 = requests.get(url + '/rankings/womens/player-rankings/odi/bowling')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page3.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    jess = []\n",
    "    for j in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        jess.append(j.text)\n",
    "    #print(jess)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = jess + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    aus = []\n",
    "    for a in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        aus.append(a.text.strip())\n",
    "    #print(aus)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = aus + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_bowlingwomen = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_bowlingwomen.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI bolwingwomen in women ----->')\n",
    "    display(Top_10_ODI_bowlingwomen)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "        \n",
    "    #PART 4 :-\n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page4 = requests.get(url + '/rankings/womens/player-rankings/odi/all-rounder')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page4.content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('tbody')]\n",
    "    #print(table)\n",
    "   \n",
    "    # Scraping player names\n",
    "    ellyse = []\n",
    "    for e in bs.find_all('div', class_='rankings-block__banner--name-large'):\n",
    "        ellyse.append(e.text)\n",
    "    #print(ellyse)\n",
    "     \n",
    "    player = [p.text for b in bs.find_all('td', class_='table-body__cell rankings-table__name name') for p in b.find_all('a')]\n",
    "    player = ellyse + player\n",
    "    #print(player)\n",
    "    \n",
    "    # Scraping the country of the player\n",
    "    aus = []\n",
    "    for a in bs.find_all('div', class_='rankings-block__banner--nationality'):\n",
    "        aus.append(a.text.strip())\n",
    "    #print(aus)\n",
    "    \n",
    "    team = [t.text for b in bs.find_all('td', class_='table-body__cell nationality-logo rankings-table__team') for t in b.find_all('span', class_='table-body__logo-text')]\n",
    "    team = aus + team\n",
    "    #print(team)\n",
    "    \n",
    "    # Scrapping the rating\n",
    "    rate = []\n",
    "    for ra in bs.find_all('div', class_='rankings-block__banner--rating'):\n",
    "        rate.append(ra.text)\n",
    "    #print(rate)\n",
    "        \n",
    "    rating = [r.text for r in bs.find_all('td', class_='table-body__cell rating')]\n",
    "    rating = rate + rating\n",
    "    #print(rating)\n",
    "    \n",
    "   # Scraping the career best rating\n",
    "    best = []\n",
    "    for be in bs.find_all('span', class_='rankings-block__career-best-text'):\n",
    "        best.append(be.text.strip())\n",
    "    #print(best)\n",
    "    \n",
    "    career_best_rating = [c.text.strip() for c in bs.find_all('td', class_='table-body__cell u-text-right u-hide-phablet')]\n",
    "    career_best_rating = best + career_best_rating\n",
    "    #print(career_best_rating)\n",
    "    \n",
    "    rank = range(1,11)\n",
    "    #Building the data frame\n",
    "    Top_10_ODI_all_rounder = pd.DataFrame({\n",
    "        'rank' : rank,\n",
    "        'player' : player[0:10],\n",
    "        'team' : team[0:10],\n",
    "        'rating' : rating[0:10],\n",
    "        'career_best_rating' : career_best_rating[0:10]\n",
    "    })\n",
    "    \n",
    "    Top_10_ODI_all_rounder.set_index('rank', inplace=True)\n",
    "    print('Top 10 ODI all rounder in women ----->')\n",
    "    display(Top_10_ODI_all_rounder)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "        \n",
    "    \n",
    "# Calling the function while specifying the URL\n",
    "function_6('https://www.icc-cricket.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7:-\n",
    "    Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-646f7e0b3614>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# Calling the function while specifying the URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mfunction_7\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://www.amazon.in/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-646f7e0b3614>\u001b[0m in \u001b[0;36mfunction_7\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m's?i=electronics&bbn=1389432031&rh=n%3A1389432031%2Cp_36%3A-2000000&dc&qid=1612616602&rnid=1318502031&ref=sr_nr_p_36_5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'apikey'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'xxx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    641\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_7(url):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page = Request(url + 's?i=electronics&bbn=1389432031&rh=n%3A1389432031%2Cp_36%3A-2000000&dc&qid=1612616602&rnid=1318502031&ref=sr_nr_p_36_5')\n",
    "    page.add_header('apikey', 'xxx')\n",
    "    content = urlopen(page).read()\n",
    "    \n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Scraping all the required data\n",
    "    table = [b.text for b in bs.find_all('div', class_=\"s-main-slot s-result-list s-search-results sg-row\" )]\n",
    "    #print(table)\n",
    "\n",
    "    # Scrapping the name and specifications\n",
    "    mobile = [m.text.strip() for m in bs.find_all('a', class_='a-link-normal a-text-normal')]\n",
    "    print(mobile)\n",
    "    \n",
    "    # Scrapping the price\n",
    "    price = [p.text for p in bs.find_all('span', class_='a-price-whole')]\n",
    "    print(price)  \n",
    "    \n",
    "    # Scrapping the image urls\n",
    "    img = [i.get('src') for b in bs.find_all('a', class_='a-link-normal s-no-outline') for i in b.find_all('img')]\n",
    "    print(img)\n",
    "    \n",
    "    # Scrapping the ratings(stars)\n",
    "    stars = [s.text.strip() for b in bs.find_all('div', class_='a-row a-size-small') for s in b.find_all('span', class_='a-icon-alt')]\n",
    "    print(stars)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Calling the function while specifying the URL\n",
    "function_7('https://www.amazon.in/')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code was blocking from scrapping the data at some point after executing several times, and so was rejecting the page request each time. \n",
    "In order to overcome this problem I used multiple request from different search engines to fetch the request of the page.\n",
    "The website sometimes displays the scraped the data or it blocks the data, only these two things are occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "17\n",
      "24\n",
      "17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mobile_name</th>\n",
       "      <th>mobile_price</th>\n",
       "      <th>mobile_img</th>\n",
       "      <th>mobile_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mobile_name, mobile_price, mobile_img, mobile_rating]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "from urllib.request import Request, urlopen\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_7(url,last_page):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    keys = ['Accept', 'Accept-Language', 'Referer', 'Sec-Fetch-Dest', 'Sec-Fetch-Site', 'Sec-Fetch-User', 'Upgrade-Insecure-Requests']\n",
    "    values = [\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "          \"en-GB,en;q=0.9,mr-IN;q=0.8,mr;q=0.7,hi-IN;q=0.6,hi;q=0.5,en-US;q=0.4\",\"https://www.scrapehero.com/\",\n",
    "          \"document\",\"cross-site\",\"?1\",\"1\"]\n",
    "    \n",
    "    # Now we will use a loop to navigate through the pages\n",
    "    #pick any random page number\n",
    "    ran_page = list(range(1,last_page))\n",
    "    random.shuffle(ran_page)\n",
    "    \n",
    "    # These are the lists which will contain all the relevant data\n",
    "    mobile_name = []\n",
    "    mobile_price = []\n",
    "    mobile_img = []\n",
    "    mobile_rating = []\n",
    "    full_list_mobile_name = []\n",
    "    full_list_mobile_price = []\n",
    "    full_list_mobile_img = []\n",
    "    full_list_mobile_rating = []\n",
    "   \n",
    "\n",
    "    # Writing a request to fetch the data from the web page \n",
    "    for page_no in ran_page:\n",
    "        if page_no == 1:\n",
    "            page = Request(url + '/s?i=electronics&bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031%2Cp_36%3A-2000000&dc&qid=1612600046&rnid=1318502031&ref=sr_nr_p_36_1')\n",
    "        else:\n",
    "            page = Request(url + '/s?i=electronics&bbn=1389401031&rh=n%3A976419031%2Cn%3A1389401031%2Cn%3A1389432031%2Cp_36%3A-2000000&dc&page=' + str(page_no) + '&qid=1612600046&rnid=1318502031&ref=sr_nr_p_36_1')\n",
    "    #print('page number ---->',ran_page,'\\n\\n')\n",
    "    \n",
    "        #pick any random key+value to get to the url and open it for fetching the data\n",
    "        ran = random.randint(0, len(keys)-1)\n",
    "        page.add_header(keys[ran], values[ran])\n",
    "    \n",
    "        toggle_switch = 'ON'\n",
    "        try:\n",
    "            page_content = urlopen(page).read()\n",
    "        except:\n",
    "            #print('The request for fetching the page content of the page', ran_page, 'has been blocked by amazon')\n",
    "            toggle_switch = 'OFF'\n",
    "    \n",
    "        if toggle_switch == 'ON':\n",
    "            \n",
    "            # Let's add time delay to our code so that the site does not sense any acticvity, as it was blocking earlier\n",
    "            time.sleep(random.randint(3,10))\n",
    "        \n",
    "            # Parsing and getting access to the content of the requested page\n",
    "            bs = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "            # Scrapping the name and specifications\n",
    "            mobile = bs.find_all('h2', class_='a-size-mini a-spacing-none a-color-base s-line-clamp-2')\n",
    "            print(len(mobile))\n",
    "    \n",
    "            # Scrapping the price\n",
    "            price = bs.find_all('span', class_='a-price-whole')\n",
    "            print(len(price))  \n",
    "    \n",
    "            # Scrapping the image urls\n",
    "            img = bs.find_all('a', class_='a-link-normal s-no-outline')\n",
    "            print(len(img))\n",
    "    \n",
    "            # Scrapping the ratings(stars)\n",
    "            stars = bs.find_all('div', class_='a-row a-size-small') \n",
    "            print(len(stars))\n",
    "        \n",
    "            for m in mobile:\n",
    "                full_list_mobile_name.append(m.text.strip('\\n'))\n",
    "        \n",
    "            for p in price:\n",
    "                full_list_mobile_price.append(p.text)\n",
    "                \n",
    "            for i in img:\n",
    "                url = i.find('img').get('src')\n",
    "                full_list_mobile_img.append(url)\n",
    "            \n",
    "            for s in stars:\n",
    "                rating = s.find('span', class_='a-icon-alt').text\n",
    "                full_list_mobile_rating.append(rating.strip('\\n'))\n",
    "    \n",
    "                if (len(price)==len(img)) and (len(img)==len(stars)):\n",
    "                    for m in mobile:\n",
    "                        mobile_name.append(m.text.strip('\\n'))\n",
    "        \n",
    "                    for p in price:\n",
    "                        mobile_price.append(p.text)\n",
    "                \n",
    "                    for i in img:\n",
    "                        url = i.find('img').get('src')\n",
    "                        mobile_img.apend(url)\n",
    "            \n",
    "                    for s in stars:\n",
    "                        rating = s.find('span', class_='a-icon-alt').text\n",
    "                        mobile_rating.append(rating.strip('\\n'))\n",
    "            \n",
    "        \n",
    "    #Building the data frame\n",
    "    amazon_phones = pd.DataFrame({\n",
    "        'mobile_name' : mobile_name,\n",
    "        'mobile_price' : mobile_price,\n",
    "        'mobile_img' : mobile_img,\n",
    "        'mobile_rating' : mobile_rating\n",
    "    })\n",
    "    \n",
    "    display(amazon_phones)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')  \n",
    "\n",
    "    \n",
    "# Calling the function while specifying the URL\n",
    "function_7('https://www.amazon.in/', 219)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8:-\n",
    "    Write a python program to extract information about the local weather from the National Weather Service website of USA, https://www.weather.gov/ for the city, San Francisco. You need to extract data about 7 day extended forecast display for the city. The data should include period, short description, temperature and description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Forecast for San Francisco CA ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>img</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overnight</td>\n",
       "      <td>newimages/medium/nbkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>Low: 45 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monday</td>\n",
       "      <td>newimages/medium/bkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>High: 58 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MondayNight</td>\n",
       "      <td>newimages/medium/nbkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>Low: 48 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>newimages/medium/bkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>High: 58 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TuesdayNight</td>\n",
       "      <td>newimages/medium/nbkn.png</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>Low: 47 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>newimages/medium/few.png</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>High: 60 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WednesdayNight</td>\n",
       "      <td>newimages/medium/nsct.png</td>\n",
       "      <td>Partly Cloudy</td>\n",
       "      <td>Low: 48 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>newimages/medium/ra.png</td>\n",
       "      <td>Rain Likely</td>\n",
       "      <td>High: 59 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ThursdayNight</td>\n",
       "      <td>newimages/medium/nra.png</td>\n",
       "      <td>Rain Likely</td>\n",
       "      <td>Low: 50 °F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           period                        img     short_desc         temp\n",
       "0       Overnight  newimages/medium/nbkn.png  Mostly Cloudy   Low: 45 °F\n",
       "1          Monday   newimages/medium/bkn.png  Mostly Cloudy  High: 58 °F\n",
       "2     MondayNight  newimages/medium/nbkn.png  Mostly Cloudy   Low: 48 °F\n",
       "3         Tuesday   newimages/medium/bkn.png  Mostly Cloudy  High: 58 °F\n",
       "4    TuesdayNight  newimages/medium/nbkn.png  Mostly Cloudy   Low: 47 °F\n",
       "5       Wednesday   newimages/medium/few.png          Sunny  High: 60 °F\n",
       "6  WednesdayNight  newimages/medium/nsct.png  Partly Cloudy   Low: 48 °F\n",
       "7        Thursday    newimages/medium/ra.png    Rain Likely  High: 59 °F\n",
       "8   ThursdayNight   newimages/medium/nra.png    Rain Likely   Low: 50 °F"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "NOTE :- The extended forecast was available for 3 to 4 days only in the above format on the whole website and so was not able to scrape the information for comming week(say 6 or 7 days), but there was some information available in the detailed forecast table present on the website, so got data scrapped from both the tables\n",
      "----------------------------------------------------------------- \n",
      "\n",
      "\n",
      "Detailed Forecast for San Francisco CA ----->\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "      <th>desc</th>\n",
       "      <th>short_desc</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overnight</td>\n",
       "      <td>Mostly cloudy, with a low around 45. West southwest wind 3 to 5 mph.</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>low around 45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monday</td>\n",
       "      <td>Mostly cloudy, with a high near 58. Calm wind becoming west southwest 5 to 7 mph in the afternoon.</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>high near 58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Monday Night</td>\n",
       "      <td>Mostly cloudy, with a low around 48. West wind 6 to 8 mph.</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>low around 48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Mostly cloudy, with a high near 58. West wind 9 to 11 mph.</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>high near 58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuesday Night</td>\n",
       "      <td>Mostly cloudy, with a low around 47. West wind 7 to 9 mph.</td>\n",
       "      <td>Mostly cloudy</td>\n",
       "      <td>low around 47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Sunny, with a high near 60.</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>high near 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wednesday Night</td>\n",
       "      <td>Partly cloudy, with a low around 48.</td>\n",
       "      <td>Partly cloudy</td>\n",
       "      <td>low around 48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>Rain likely, mainly after 4pm.  Mostly cloudy, with a high near 59.</td>\n",
       "      <td>Rain likely, mainly after 4pm.  Mostly cloudy</td>\n",
       "      <td>high near 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thursday Night</td>\n",
       "      <td>Rain likely, mainly before 10pm.  Mostly cloudy, with a low around 50.</td>\n",
       "      <td>Rain likely, mainly before 10pm.  Mostly cloudy</td>\n",
       "      <td>low around 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Friday</td>\n",
       "      <td>A chance of rain, mainly between 10am and 4pm.  Partly sunny, with a high near 59.</td>\n",
       "      <td>A chance of rain, mainly between 10am and 4pm.  Partly sunny</td>\n",
       "      <td>high near 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Friday Night</td>\n",
       "      <td>A chance of rain.  Mostly cloudy, with a low around 49.</td>\n",
       "      <td>A chance of rain.  Mostly cloudy</td>\n",
       "      <td>low around 49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>Rain likely.  Mostly cloudy, with a high near 58.</td>\n",
       "      <td>Rain likely.  Mostly cloudy</td>\n",
       "      <td>high near 58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Saturday Night</td>\n",
       "      <td>A chance of rain.  Mostly cloudy, with a low around 46.</td>\n",
       "      <td>A chance of rain.  Mostly cloudy</td>\n",
       "      <td>low around 46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>A chance of rain.  Partly sunny, with a high near 57.</td>\n",
       "      <td>A chance of rain.  Partly sunny</td>\n",
       "      <td>high near 57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             period  \\\n",
       "0         Overnight   \n",
       "1            Monday   \n",
       "2      Monday Night   \n",
       "3           Tuesday   \n",
       "4     Tuesday Night   \n",
       "5         Wednesday   \n",
       "6   Wednesday Night   \n",
       "7          Thursday   \n",
       "8    Thursday Night   \n",
       "9            Friday   \n",
       "10     Friday Night   \n",
       "11         Saturday   \n",
       "12   Saturday Night   \n",
       "13           Sunday   \n",
       "\n",
       "                                                                                                   desc  \\\n",
       "0                                 Mostly cloudy, with a low around 45. West southwest wind 3 to 5 mph.    \n",
       "1   Mostly cloudy, with a high near 58. Calm wind becoming west southwest 5 to 7 mph in the afternoon.    \n",
       "2                                           Mostly cloudy, with a low around 48. West wind 6 to 8 mph.    \n",
       "3                                           Mostly cloudy, with a high near 58. West wind 9 to 11 mph.    \n",
       "4                                           Mostly cloudy, with a low around 47. West wind 7 to 9 mph.    \n",
       "5                                                                           Sunny, with a high near 60.   \n",
       "6                                                                  Partly cloudy, with a low around 48.   \n",
       "7                                   Rain likely, mainly after 4pm.  Mostly cloudy, with a high near 59.   \n",
       "8                                Rain likely, mainly before 10pm.  Mostly cloudy, with a low around 50.   \n",
       "9                    A chance of rain, mainly between 10am and 4pm.  Partly sunny, with a high near 59.   \n",
       "10                                              A chance of rain.  Mostly cloudy, with a low around 49.   \n",
       "11                                                    Rain likely.  Mostly cloudy, with a high near 58.   \n",
       "12                                              A chance of rain.  Mostly cloudy, with a low around 46.   \n",
       "13                                                A chance of rain.  Partly sunny, with a high near 57.   \n",
       "\n",
       "                                                      short_desc  \\\n",
       "0                                                  Mostly cloudy   \n",
       "1                                                  Mostly cloudy   \n",
       "2                                                  Mostly cloudy   \n",
       "3                                                  Mostly cloudy   \n",
       "4                                                  Mostly cloudy   \n",
       "5                                                          Sunny   \n",
       "6                                                  Partly cloudy   \n",
       "7                  Rain likely, mainly after 4pm.  Mostly cloudy   \n",
       "8                Rain likely, mainly before 10pm.  Mostly cloudy   \n",
       "9   A chance of rain, mainly between 10am and 4pm.  Partly sunny   \n",
       "10                              A chance of rain.  Mostly cloudy   \n",
       "11                                   Rain likely.  Mostly cloudy   \n",
       "12                              A chance of rain.  Mostly cloudy   \n",
       "13                               A chance of rain.  Partly sunny   \n",
       "\n",
       "      temperature  \n",
       "0   low around 45  \n",
       "1    high near 58  \n",
       "2   low around 48  \n",
       "3    high near 58  \n",
       "4   low around 47  \n",
       "5    high near 60  \n",
       "6   low around 48  \n",
       "7    high near 59  \n",
       "8   low around 50  \n",
       "9    high near 59  \n",
       "10  low around 49  \n",
       "11   high near 58  \n",
       "12  low around 46  \n",
       "13   high near 57  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a function which will take a URL each time the function is called\n",
    "def function_8(url):\n",
    "    \n",
    "    # Writing a request to fetch the data from the web page\n",
    "    page = requests.get(url + 'MapClick.php?lon=-122.41323847509919&lat=37.77766187310338#.YB-ZNugzZPY')\n",
    "    \n",
    "    # Parsing and getting access to the content of the requested page\n",
    "    bs = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # First we will scrape the data from the extended forecast present on the page and then we will scrape the detailed forecast info present on that page.\n",
    "    # In this way we will get the forecast for the coming week aswell\n",
    "    # Scrapping all the required data from the extended forecast text\n",
    "    table = [b.text for b in bs.find_all('div', id='seven-day-forecast-container')]\n",
    "    #print(table)\n",
    "    \n",
    "    # Scrapping the period \n",
    "    period = [p.text for b in bs.find_all('li', class_='forecast-tombstone') for p in b.find_all('p', class_='period-name')]\n",
    "    #print(period)\n",
    "    \n",
    "    # Scrapping the image url\n",
    "    img = [i.get('src') for b in bs.find_all('li', class_='forecast-tombstone') for d in b.find_all('div', class_='tombstone-container') for i in d.find_all('img')]\n",
    "    #print(img)\n",
    "    \n",
    "    # Scrapping the short description\n",
    "    short_desc = [sd.text  for sd in bs.find_all('p', class_='short-desc')]\n",
    "    #print(short_desc)\n",
    "    \n",
    "    # Scrapping the temperature\n",
    "    temp = [t.text for t in bs.find_all('p', class_='temp')]\n",
    "    #print(temp)\n",
    "    \n",
    "    #Building the data frame\n",
    "    extended_forecast = pd.DataFrame({\n",
    "        'period' : period,\n",
    "        'img' : img,\n",
    "        'short_desc' : short_desc,\n",
    "        'temp' : temp\n",
    "    })\n",
    "    \n",
    "    print('Extended Forecast for San Francisco CA ----->')\n",
    "    display(extended_forecast)\n",
    "    print('-----------------------------------------------------------------','\\n\\n')\n",
    "    print('NOTE :- The extended forecast was available for 3 to 4 days only in the above format on the whole website and so was not able to scrape the information for comming week(say 6 or 7 days), but there was some information available in the detailed forecast table present on the website, so got data scrapped from both the tables')\n",
    "    print('-----------------------------------------------------------------','\\n\\n') \n",
    "    \n",
    "    # Scrapping all the required data from the detailed forecast\n",
    "    table = [b.text for b in bs.find_all('div', id='detailed-forecast', class_='panel panel-default')]\n",
    "    #print(table)\n",
    "    \n",
    "    # Scrapping the period\n",
    "    period = [p.text for b in bs.find_all('div', class_='col-sm-2 forecast-label') for p in b.find_all('b')]\n",
    "    #print(period)\n",
    "     \n",
    "    # Scrapping the description \n",
    "    desc = [d.text for d in bs.find_all('div', class_='col-sm-10 forecast-text')]\n",
    "    #print(desc)\n",
    "     \n",
    "    # Now we need to seperate the information from the description\n",
    "    # We will be using regex to do the task\n",
    "    # Extract the short description\n",
    "    import re\n",
    "    short_desc = []\n",
    "    short_description = []\n",
    "    for s in desc:\n",
    "        short_desc.append(re.findall('^.*?(?=with)',s))#'([a-zA-Z]+?)\\s*([a-zA-Z]+?),'\n",
    "    #print(short_desc)\n",
    "    #for list_to_str in short_desc:\n",
    "        #for a in list_to_str:\n",
    "            #temp = ' '.join([str(elem) for elem in a])\n",
    "            #print(temp)\n",
    "            #short_description.append(temp)\n",
    "    for list_to_str in short_desc:\n",
    "        for a in list_to_str:\n",
    "            strip = a.strip('[]')\n",
    "            strip = strip.strip(' ,')\n",
    "            #print(strip)\n",
    "            short_description.append(strip)\n",
    "    \n",
    "    # Extract temperature\n",
    "    temp_F = []\n",
    "    temperature = []\n",
    "    for t in desc:\n",
    "        temp_F.append(re.findall('with.+?\\S+?[0-9].', t))\n",
    "    #print(temp_F)\n",
    "    for temp_to_strip in temp_F:\n",
    "        for b in temp_to_strip:\n",
    "            strip = b.strip('with.')\n",
    "            strip = strip.strip('a ')\n",
    "            #print(strip)\n",
    "            temperature.append(strip)\n",
    "            \n",
    "    \n",
    "    #Building the data frame\n",
    "    detailed_forecast = pd.DataFrame({\n",
    "        'period' : period,\n",
    "        'desc' : desc,\n",
    "        'short_desc' : short_description,\n",
    "        'temperature' : temperature\n",
    "    })\n",
    "    \n",
    "    print('Detailed Forecast for San Francisco CA ----->')\n",
    "    display(detailed_forecast)\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print('-----------------------------------------------------------------','\\n\\n') \n",
    "\n",
    "# Calling the function while specifying the URL\n",
    "function_8('https://forecast.weather.gov/')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
